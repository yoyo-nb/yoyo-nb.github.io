<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>【算法笔记】目标检测之YOLO系列v1v2v3 | YOYO</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script></head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><body><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">YOYO</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">YOYO</a></h1></div><p class="m-desc">一只小菜鸡,<br>希望自己更牛逼</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">【算法笔记】目标检测之YOLO系列v1v2v3</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/post/79989519.html">2021-03-09</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/算法笔记/">算法笔记</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p><img src="/images/v2-181f533e731b216a98b238af42ffbb2b_r.jpg" alt="picture 3"></p>
<h1 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>1、YOLO是一阶段的方法，速度比RCNN系列更快。</li>
<li>2、YOLO在CNN中计算了图像中的全部特征，而RCNN在分类时系列只看到了局部特征。即YOLO使用全图作为 Context 信息，背景错误（把背景错认为物体）比较少。</li>
</ul>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ul>
<li>1、<strong>Resize</strong>成448*448，图片分割得到7*7网格(cell)</li>
<li>2、<strong>CNN提取特征和预测</strong>：卷积提取特征。全链接部分负责预测。</li>
<li>3、过滤bbox（通过<strong>nms</strong>）</li>
</ul>
<h2 id="输入与输出"><a href="#输入与输出" class="headerlink" title="输入与输出"></a>输入与输出</h2><p>图片缩放到448*448的大小，因为网络结构最后欧全连接层</p>
<p><img src="/images/bc031ac7178ce4904c91c4e0121970ca983948b9afc567d2a41987312e609166.png" alt="picture 3">  </p>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>YOLO将输入图像分成SxS（S=7）个格子，每个格子负责检测‘落入’该格子的物体。若某个物体的中心位置的坐标落入到某个格子，那么这个格子就负责检测出这个物体。</p>
<p><img src="/images/79d94534d4eaf24e08b60215b7facc8c52a5ab1e125433eb640ad5ca57f57699.png" alt="picture 1">  </p>
<h3 id="bounding-box"><a href="#bounding-box" class="headerlink" title="bounding box"></a>bounding box</h3><p>为每个格子预先设置B个bounding box（B=2）。则每个格子的输出要包含B个bounding box的信息，包括x,y,w,h,和confidence。<br>其中x,y代表在该格子内的相对偏移，取值[0,1]。w,h取值可以大于1。</p>
<p>bounding box的置信度Label计算方式为:</p>
<script type="math/tex; mode=display">
\text { Confidence }=\operatorname{Pr}(\text { Object }) * I O U_{\text {pred }}^{\text {truth }}</script><ul>
<li>$\operatorname{Pr}(\text { Object })$代表该bounding box中是否包含物体。若包含物体，则P(object) = 1；否则P(object) = 0。 </li>
<li>$I O U_{\text {pred }}^{\text {truth }}$为预测的bounding box与真实bounding box的接近程度。</li>
<li>Label中的IOU是在训练阶段通过ground truth计算的。在测试时无法计算，也无需计算。</li>
</ul>
<h3 id="输出结构"><a href="#输出结构" class="headerlink" title="输出结构"></a>输出结构</h3><p>YOLO原来需要检测图片中的20中物体，所以要进行20类分类，输出结构如图所示：<br><img src="/images/2f7a00b57168be8545ab1da6a13d3b2ee566b5e3642bda49797b33a9b59df232.png" alt="picture 2">  </p>
<p>所以输出是一个 7*7*30 的张量</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>YOLO检测网络包括24个卷积层和2个全连接层，如下图所示。<br><img src="/images/b4c405315fee64b85cc8e6dfef6db46293153885c5be3b2af7998ad32b75f5a7.png" alt="picture 7">  </p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/images/764b6b5c72f098d84142127ca2cb0d3ff9758e1568ea4b0d3236c52b8cce6e86.png" alt="picture 8">  </p>
<p>最上部分为坐标误差，中间部分为IOU误差，最后部分为分类误差</p>
<p>其中w与h需要先开根号再使用均方误差，原因如下:<br>对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏一点更不能忍受。在下图中small bbox的横轴值较小，发生偏移时，反应到y轴上的loss（下图绿色）比big box(下图红色)要大。<br><img src="/images/5e6f1342a0383c8904deeca99f3cf68a0fde7299592ec518f5509026c1fc04f3.png" alt="picture 9">  </p>
<hr>
<h1 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h1><h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/images/ab0f956dee2cd91570330f981a0f1d555e4f3df25f9d3d9f015676b50729b7e9.png" alt="picture 12"> </p>
<h2 id="一系列改进"><a href="#一系列改进" class="headerlink" title="一系列改进"></a>一系列改进</h2><h3 id="1、引入BN"><a href="#1、引入BN" class="headerlink" title="1、引入BN"></a>1、引入BN</h3><p>在每一个卷积层后添加batch normalization，极大的改善了收敛速度同时减少了对其它regularization方法的依赖（舍弃了dropout依然没有过拟合），使得mAP获得了2%的提升。</p>
<h3 id="2、高分辨率预训练"><a href="#2、高分辨率预训练" class="headerlink" title="2、高分辨率预训练"></a>2、高分辨率预训练</h3><p>YOLOv1在训练时先使用224x224的图片输入来预训练自己的特征提取网络，再使用448x448的数据集进行finetune。这说明网络需要重新学习识别大尺度（448）的图片以及学习进行其上的目标检测工作。</p>
<p>YOLOv2首先修改预训练分类网络的分辨率为448*448，在ImageNet数据集上训练10轮（10 epochs）。这个过程让网络有足够的时间调整filter去适应高分辨率的输入。然后fine tune为检测网络。mAP获得了4%的提升。</p>
<h3 id="3、使用anchor-box"><a href="#3、使用anchor-box" class="headerlink" title="3、使用anchor box"></a>3、使用anchor box</h3><p>YOLO(v1)使用全连接层数据进行bounding box预测（要把1470<em>1的全链接层reshape为7</em>7*30的最终特征），这会丢失较多的空间信息定位不准。</p>
<ul>
<li>移除全连接层</li>
<li>去掉最后的池化层确保输出的卷积特征图有更高的分辨率。</li>
<li>缩减网络，让图片输入分辨率为416 * 416，目的是让后面产生的卷积特征图宽高都为奇数</li>
<li>使用卷积层降采样（factor 为32），最终得到13 * 13的卷积特征图（416/32=13）。</li>
<li>把预测类别的机制从空间位置(cell)中解耦，由9个anchor box同时预测类别和坐标。</li>
</ul>
<p><img src="/images/2d133c1dd9c168c75cb4579c3604fb2a52fa7f5d7a16a955147a5eef36db6793.png" alt="picture 10">  </p>
<p>共会预测13 <em> 13 </em> 9 = 1521个boxes，而之前的网络仅仅预测7 <em> 7 </em> 2 = 98个boxes。</p>
<h3 id="4、Kmeans寻找预设anchors形状"><a href="#4、Kmeans寻找预设anchors形状" class="headerlink" title="4、Kmeans寻找预设anchors形状"></a>4、Kmeans寻找预设anchors形状</h3><p>选择了更好的、更有代表性的先验boxes维度，那么网络就应该更容易学到准确的预测位置。<br>使用K-means聚类方法，通过对数据集中的ground truth box做聚类，找到ground truth box的统计规律。<br>若使用欧式距离，则大boxes比小boxes产生更多error。使用以下距离：</p>
<script type="math/tex; mode=display">d(\text { box }, \text { centroid })=1-I O U(\text { box, centroid })</script><p>最终选择k=5。</p>
<h3 id="5、预测相对位置"><a href="#5、预测相对位置" class="headerlink" title="5、预测相对位置"></a>5、预测相对位置</h3><p>由于借鉴了Faster-RCNN的anchor，但使用Faster-RCNN的位置回归计算方式不容易收敛，因为$x=\left(t_{x} <em> w_{a}\right)+x_{a}, y=\left(t_{y} </em> h_{a}\right)+y_{a}$,这个公式没有任何限制，无论在什么位置进行预测，任何anchor boxes可以在图像中任意一点。<br>所以要预测相对位置，网络在特征图（13 *13 ）的每个cell上预测5个bounding boxes，每一个bounding box预测5个坐标值：tx，ty，tw，th，to。如果这个cell距离图像左上角的边距为（cx，cy）以及该cell对应的box的长和宽分别为（pw，ph），那么对应的box为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
b_{x} &=\sigma\left(t_{x}\right)+c_{x} \\
b_{y} &=\sigma\left(t_{y}\right)+c_{y} \\
b_{w} &=p_{w} e^{t_{w}} \\
b_{h} &=p_{h} e^{t_{h}} \\
\operatorname{Pr}(\text { object }) * I O U(b, \text { object }) &=\sigma\left(t_{o}\right)
\end{aligned}</script><p><img src="/images/919ee5fda3ab9c5bc34103b3da43187009c3ec682ba64f48ae689bc421b0a927.png" alt="picture 11">  </p>
<h3 id="6、细粒度特征"><a href="#6、细粒度特征" class="headerlink" title="6、细粒度特征"></a>6、细粒度特征</h3><p>在不同层次的特征图上产生区域建议以获得多尺度的适应性。<br>YOLO v2使用passthrough layer，将倒数第二层特征图（26<em>26）叠加相邻特征到不同通道，把26 </em> 26 <em> 512的特征图叠加成13 </em> 13 * 2048的特征图，与原生的深层特征图相连接。</p>
<p><img src="/images/4d20d94fb2100a6cd77c587828a47879c5c2d1ff1da9b4452efc8a01d0a5acd8.png" alt="picture 2">  </p>
<h3 id="7、多尺度训练"><a href="#7、多尺度训练" class="headerlink" title="7、多尺度训练"></a>7、多尺度训练</h3><p>由于网络取消了全连接层，所以可以使用多种分辨率的输入图片进行训练。</p>
<h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/images/591302b840137bf0e9d748bc7ee6a0460f3cfc60168c15a41be1b8c87f6cea61.png" alt="picture 3">  </p>
<ul>
<li>第一行计算的是当前anchor box内无物体时，希望预测结果为0。（也就是背景置信度误差）</li>
<li>第二行为预测结果与预设anchor box形状的误差，让网络从启动后快速学到anchor的形状的过程，只在前12800次迭代正起作用。</li>
<li>第三行为<strong>坐标损失</strong>预测结果与ground truth的误差</li>
<li>第四行为<strong>置信度损失</strong>，也就是每个anchor box的conferen的损失</li>
<li>第五行为<strong>分类损失</strong>，均方误差与YOLO v1一样。</li>
<li>其中第2行需要变为$(2-w_i<em>h_i)</em>(prior_k^r-b_{ijk}^r)$，第三行同理。原因与YOLO v1相同，大目标对wh约束不强，小目标更严格。而YOLO v1中的开根号方式作者认为没用。所以改用这样的形式。</li>
</ul>
<hr>
<h1 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h1><h2 id="网络结构-2"><a href="#网络结构-2" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/images/e4b858710199a4233518d9af0171daa965a3be843878cfba0c9f1eab985e0607.png" alt="picture 13">  </p>
<p><img src="/images/e084250da679132aa454eccdc115e72d4ec1ed8be4fd7872177cab7d3c764b25.png" alt="picture 1">  </p>
<p>此结构主要由75个卷基层构成，没有池化层和全连接层，张量的尺寸变换是通过改变卷积核的步长来实现的。<br>yolo_v2中对于前向过程中张量尺寸变换，都是通过 最大池化来进行，一共有5次。而v3是通过卷积核 增大步长来进行，也是5次。</p>
<p>网络预测部分类似FPN结构：<br><img src="/images/3b8f4f275b9260527b828cb545088f89f650107954f9150fdc3ecf5940c59823.png" alt="picture 4">  </p>
<h2 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h2><p>每个输出都为255维向量，规模分别为13，26，52为在不同尺度上进行预测的结果。输出使用3个bounding box，COCO数据集共80类，每个box有（x，y，w，h，conference）信息，所以（80+5）*3=255。</p>
<p>与YOLOv2一样，YOLOv3也是在feature map上对每个位置进行bbox预测。预测相对当前grid的相对值，分别是(tx,ty,tw,th)。最终的预测bbox为：bx,by,bw,bh，这是在image的bbox。（唯一不同是YOLOv2用5个box）</p>
<h2 id="替换softmax层：对应多重label分类"><a href="#替换softmax层：对应多重label分类" class="headerlink" title="替换softmax层：对应多重label分类"></a>替换softmax层：对应多重label分类</h2><p>Softmax层被替换为一个1x1的卷积层+logistic激活函数的结构。使用softmax层的时候其实已经假设每个输出仅对应某一个单个的class，但是在某些class存在重叠情中，使用softmax就不能使网络对数据进行很好的拟合。</p>
<h2 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/images/64bd2aeef1de28784d899d5b5432040612023cf29ee4dba87d9764e3108264d5.png" alt="picture 5">  </p>
<ul>
<li>上面两行为bounding box预测误差，与YOLO v2相同。</li>
<li>第三行第四行为bounding box 的conference的二元交叉熵误差，第三行为有物体情况，第四行为无物体情况。由于图片中大部分区域无物体，所以无物体情况乘衰减系数$\lambda_{noobj}$，比如0.5</li>
<li>第五行为分类误差，使用二元交叉熵。</li>
</ul>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/46691043" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46691043</a><br><a href="https://zhuanlan.zhihu.com/p/24916786" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24916786</a><br><a href="https://zhuanlan.zhihu.com/p/25236464/" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25236464/</a><br><a href="https://blog.csdn.net/c20081052/article/details/80236015" target="_blank" rel="noopener">https://blog.csdn.net/c20081052/article/details/80236015</a><br><a href="https://zhuanlan.zhihu.com/p/25167153" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25167153</a><br><a href="https://blog.csdn.net/leviopku/article/details/82660381" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/82660381</a><br><a href="https://zhuanlan.zhihu.com/p/40332004" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40332004</a><br><a href="https://blog.csdn.net/just_sort/article/details/103232484" target="_blank" rel="noopener">https://blog.csdn.net/just_sort/article/details/103232484</a><br><a href="https://zhuanlan.zhihu.com/p/142408168" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/142408168</a></p>
</div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tag"></i><a href="/tags/目标检测/">目标检测</a></span></div></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/post/5a444702.html">&lt; 【算法笔记】语义分割之FCN与Mask-RCNN</a><a class="next" href="/post/205a2488.html">【算法笔记】目标检测之Fast RCNN与Faster RCNN &gt;</a></div><div id="valine-comment"><style type="text/css">.v * { color: #CECECE; }
.v a { color: #0F9FB4; }
.v a:hover { color: #216C73; }
.v li { list-style: inherit; }
.v .vwrap { border: 1px solid #223441; border-radius: 0; }
.v .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.v .vbtn { border-radius: 0; color: #cecece; background: none; }
.v .vlist .vcard .vh { border-bottom-color: #293D4E; }
.v .vwrap .vheader .vinput { border-bottom-color: #223441; }
.v .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.v code, .v pre,.v .vlist .vcard .vhead .vsys { background: #203240; }
.v .vlist .vcard .vcontent.expand:before { background: linear-gradient(180deg,hsla(206,33%,19%,0),hsla(206,33%,19%,.9)); }
.v .vlist .vcard .vcontent.expand:after { background: hsla(206,33%,19%,.9); }</style><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@1.4.0/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'faWhCflfYAf4DSBY6i31K1r7-gzGzoHsz',
  appKey:'CKuisu7G1GD4VMH48mNdbWjX',
  placeholder:'无需注册，随便说的什么吧，留下邮箱的话，被回复会有邮件提醒哦ヾ(o◕∀◕)ﾉヾ',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script></section><footer><p>Copyright © 2016 - 2021 <a href="/." rel="nofollow">YOYO</a><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"></body></html>