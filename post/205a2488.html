<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>【算法笔记】目标检测之Fastst RCNN与Faster RCNN | YOYO</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script></head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><body><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">YOYO</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">YOYO</a></h1></div><p class="m-desc">一只小菜鸡,<br>希望自己更牛逼</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">【算法笔记】目标检测之Fastst RCNN与Faster RCNN</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/post/205a2488.html">2021-03-07</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/算法笔记/">算法笔记</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p><img src="/images/12.jpg" alt="1">  </p>
<h1 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h1><ul>
<li>RCNN对每个region都进行CNN提取特征，由于region重叠部分较多，使CNN对相同区域提取多次，耗费时间。</li>
<li>并且RCNN提取region特征之前需要resize，破坏图中内容的结构，影响效果。</li>
</ul>
<p>所以Fast R-CNN诞生了。</p>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p><img src="/images/4efe5f0398d1155f780581e60fcb227ce4af794baf8a9a6efac1f947acca20e4.png" alt="picture 3"><br><img src="/images/91920bcf17c49974aa9f64fe1f934c79e0dac19a53f5e69c1f7cf39c2f7f28ae.png" alt="picture 4">  </p>
<ul>
<li><p>如图，Fast R-CNN将整张图片resize送入神经网络（RCNN是对每个region进行resize），在最后一层再加入候选框信息（这些候选框还是经过Selective Search提取，再将在原图中的位置映射到最后一层特征图上），这样CNN提取特征只需对整张图做一次提取即可。</p>
</li>
<li><p>使用ROI pooling layer对最后一层特征图中每个region的对应的区域进行pooling，产生固定大小的输出，再送进FC层。</p>
</li>
<li><p>FC层的输出两部分，使用了多任务损失函数(multi-task loss)。</p>
</li>
</ul>
<h2 id="ROI-pooling-layer"><a href="#ROI-pooling-layer" class="headerlink" title="ROI pooling layer"></a>ROI pooling layer</h2><p>作用：对任意大小的输入产生固定的输出</p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ol>
<li><p>提取特征的网络的最后一个特征图层；</p>
</li>
<li><p>一个表示图片中所有ROI的N*5的矩阵，其中N表示ROI（region of interest）的数目。第一列表示图像index，其余四列表示坐标（x,y,h,w）。坐标的参考系不是针对feature map的。其中x，y表示RoI的左上角在整个图片中的坐标。</p>
</li>
</ol>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>输出为，与ROI同数量的vector，vector大小为channel<em>w</em>h。channel为最后一层feature map的channel数。w，h为约定好的固定大小的输出。</p>
<h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><ul>
<li>1、根据输入image，将ROI坐标映射到feature map对应位置</li>
<li>将映射后的区域划分为相同大小的区域，区域数量与约定好的输出数量相同。</li>
<li>对每个区域进行max pooling操作</li>
</ul>
<p>具体计算过程如下动图所示（假设输出w，h为2*2）：<br><img src="/images/20181110170648243.gif" alt="picture1"></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/images/0a38f343339dc7fcc9fddb0fa3f9913a488af2570eb77489c943bd7adf72bc22.png" alt="picture 5"><br><img src="/images/2e7e9dd3f5a8e2629914de9ea3360339c6a59ca8003535f902ac6dfea6096516.png" alt="picture 6"><br>smooth的图像为：<br><img src="/images/7d07bffc5972ad9013cfd990e8b9bd20dc8bbd67da21c92260947cafae861ac9.png" alt="picture 7">  </p>
<hr>
<h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>Fast R-CNN与R-CNN一样需要预先Selective Search，该过程速度较慢。<br>所以Faster R-CNN将region提取的过程融入进网络里。</p>
<h2 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h2><p><img src="/images/0a1d681b4d6d86fc9b1bf4f912f74e7fb5794c550f2df807dc0d419428356993.png" alt="picture 8">  </p>
<p>从图中可以看出，该网络结构大部分与Fast R-CNN相同，唯一不同的是最后一层卷积的输出feature map输入给了<strong>RPN</strong>网络来生成region位置，并使用生成的位置和最后一层feature map作为ROI pooling layer的输入。</p>
<h2 id="区域生成网络-（-Region-Proposal-Networks-）"><a href="#区域生成网络-（-Region-Proposal-Networks-）" class="headerlink" title="区域生成网络 （ Region Proposal Networks ）"></a>区域生成网络 （ Region Proposal Networks ）</h2><p>RPN可以帮我们找出可能包含物体的那些区域</p>
<p>RPN使用固定大小的anchor，这些anchor将会均匀地放置在整个原始图像中。不同于原来我们要检测物体在哪里，我们现在利用anchor将问题转换为另外两部分：</p>
<ul>
<li>某个框内是否含有物体</li>
<li>某个框是否框的准，如果框的不准我们要如何调整框</li>
</ul>
<h3 id="anchors"><a href="#anchors" class="headerlink" title="anchors"></a>anchors</h3><p>如果直接学习物体边框$(x_{min},x_{max},y_{min},y_{max})$是困难的：</p>
<ul>
<li>边框数量不定，网络很难输出变长的数据</li>
<li>物体是不同大小有不同的宽高比，那训练一个效果很好的检测模型将会是非常复杂的</li>
<li>会存在一些无效的预测，比如$x_{max}&lt;x_{min}$时</li>
</ul>
<p>所以RPN使用anchors的方法。RPN在feature map对应的原图位置上，以每个点为中心上使用9个anchors:</p>
<ul>
<li>三种面积{128,256,512}</li>
<li>三种比例{1:1,1:2,2:1}</li>
</ul>
<p>如下图所示：<br><img src="/images/eddeed3b1712669742b5754910bebdc3effb65c024501b099a583dcc8077bbf4.png" alt="picture 1">  </p>
<p>最终将学习每个anchor中是否有东西，以及anchor坐标的偏移（学习偏移比直接学习边框位置简单）。<br>anchor位置：$(x_{center},y_{center},width,height)$</p>
<p>学习的偏移：$(\Delta x_{center},\Delta y_{center},\Delta width,\Delta height)$</p>
<p>修正过程：（g为ground-truth，a为anchor）</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta x_{\text {center }}&=\left(x_{g}-x_{a}\right) \times \text { width }_{a}\\
 \Delta y_{\text {center }}&=\left(y_{g}-y_{a}\right) \times \text { height }_{a} \\
\Delta \text { width }&=\log \left(\text { width }_{g} / \text { width }_{a}\right)\\
 \Delta \text { height }&=\log \left(\text { height }_{g} / \text { height }_{a}\right)
\end{aligned}</script><h3 id="输入与输出"><a href="#输入与输出" class="headerlink" title="输入与输出"></a>输入与输出</h3><p>我们使用第一部分返回的feature map进行输入，以全卷积的方式实现RPN。我们首先使用一个3*3*512的卷积层，然后使用两个并行的1*1的卷积内核，其中两个并行卷积层的通道数取决于每个点的Anchor数量（k个）。</p>
<p>如图所示：<br><img src="/images/3104a00f7fd0e1752a4b0b636a321c5eb0ae3009472d43b469c9a07ae334ec8c.png" alt="picture 2">  </p>
<ul>
<li>第一部分为分类层（左侧），我们为每个Anchor输出2个预测值：<strong>背景得分以及前景得分</strong>（实际含有物体）。</li>
<li>第二部分为回归层，输出4个预测值：为anchor的偏移值$(\Delta x_{center},\Delta y_{center},\Delta width,\Delta height)$</li>
</ul>
<p>卷积过程如下图所示：<br><img src="/images/d16ffcec8f681ee0840b1367a0c559ee6ead04f6258f01c44ab24d9964e17d11.png" alt="picture 3">  </p>
<ul>
<li>第一个3<em>3卷积相当于将每个3\</em>3的滑动窗口转化为一个1维向量代表该滑动窗口内的特征。该滑动窗口的感受野包括了9个以该点位中心的anchor的面积。该卷积需要1*1 padding，即边缘部分的滑动窗口也需要计算。</li>
<li>之后的两个1*1卷积相当于FC层，相当于将每个滑动窗口的特征向量进行计算，得到该滑动窗口内的9个anchor的输出值。</li>
<li>两次卷积的输出的w,h不变，代表该这些滑动窗口的计算结果。</li>
</ul>
<h3 id="RPN网络的训练以及损失函数"><a href="#RPN网络的训练以及损失函数" class="headerlink" title="RPN网络的训练以及损失函数"></a>RPN网络的训练以及损失函数</h3><h4 id="监督标签"><a href="#监督标签" class="headerlink" title="监督标签"></a>监督标签</h4><p>与ground-truth的IoU大于0.7的Anchor视为前景（如果没有就找一个与ground-truth的IoU最大的anchor），而与真实物体IoU小于0.3的视为背景。而IoU处于两个阈值中间的则忽略。</p>
<script type="math/tex; mode=display">
p *=\left\{\begin{array}{ll}
1, & I o U>0.7 \text { or max IoU for ground }-\text { truth } \\
0, & I o U<0.3 \\
\text { ignore, } & 0.3<=I o U<=0.7
\end{array}\right.</script><ul>
<li>每个ground-turth可能有多个anchor对应</li>
<li>打上正标签的anchor，将对应的groud-truth位置作为回归学习的目标</li>
</ul>
<h4 id="平衡正负样本"><a href="#平衡正负样本" class="headerlink" title="平衡正负样本"></a>平衡正负样本</h4><p>由于图中大部分区域都是背景，所以负标签较多。则每次使用mini-batch采样训练（一般为256）。每次采样128正样本anchor，并采样同数量负样本。（若正样本不足128，则采样同数量负样本）。</p>
<h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><ul>
<li>对于分类损失，通过二分类交叉熵进行计算</li>
<li>对应回归损失，只会针对那些正样本进行回归，具体来说就是对偏移进行回归$(\Delta x_{center},\Delta y_{center},\Delta width,\Delta height)$。损失函数使用<strong>Smooth L1损失</strong>。</li>
</ul>
<h3 id="RPN的输出处理"><a href="#RPN的输出处理" class="headerlink" title="RPN的输出处理"></a>RPN的输出处理</h3><ul>
<li>取所有输出为正的region的得分前M个region作为输出。</li>
<li>clip限定超出图像边界的前景anchor作为图像边界<br><img src="/images/29c3b60a0aea9bfae774351aabf49e474e72db2e8e6b267ab3968728b7715f8b.png" alt="picture 4">  </li>
<li>忽略掉长或者宽太小的建议框</li>
<li>进行NMS</li>
<li>最后重新再按得分排序，取得分前N个框</li>
</ul>
<h2 id="Faster-RCNN整体训练过程"><a href="#Faster-RCNN整体训练过程" class="headerlink" title="Faster-RCNN整体训练过程"></a>Faster-RCNN整体训练过程</h2><p><img src="/images/2f9231a620f472cf29f177a9a645543758b7a40c0f22053aa0992c12a6528972.png" alt="picture 7">  </p>
<p>第一步：用ImageNet模型初始化，独立训练一个RPN网络；<br>第二步：仍然用ImageNet模型初始化，但是使用上一步RPN网络产生的proposal作为输入，训练一个Fast-RCNN网络，至此，两个网络每一层的参数完全不共享；<br>第三步：使用第二步的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的learning rate设置为0，也就是不更新，仅仅更新RPN特有的那些网络层，重新训练，此时，两个网络已经共享了所有公共的卷积层；<br>第四步：仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个unified network，继续训练，fine tune Fast-RCNN特有的网络层，此时，该网络已经实现我们设想的目标，即网络内部预测proposal并实现检测的功能。</p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/24780395" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24780395</a><br><a href="https://zhuanlan.zhihu.com/p/24916624" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24916624</a><br><a href="https://zhuanlan.zhihu.com/p/123962549" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/123962549</a><br><a href="https://blog.csdn.net/w437684664/article/details/104238521" target="_blank" rel="noopener">https://blog.csdn.net/w437684664/article/details/104238521</a><br><a href="https://www.cnblogs.com/jiangnanyanyuchen/p/9433791.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangnanyanyuchen/p/9433791.html</a></p>
</div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tag"></i><a href="/tags/目标检测/">目标检测</a></span></div></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/post/79989519.html">&lt; 【算法笔记】目标检测之YOLO系列v1v2v3</a><a class="next" href="/post/664697e9.html">【算法笔记】目标检测之RCNN &gt;</a></div><div id="valine-comment"><style type="text/css">.v * { color: #CECECE; }
.v a { color: #0F9FB4; }
.v a:hover { color: #216C73; }
.v li { list-style: inherit; }
.v .vwrap { border: 1px solid #223441; border-radius: 0; }
.v .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.v .vbtn { border-radius: 0; color: #cecece; background: none; }
.v .vlist .vcard .vh { border-bottom-color: #293D4E; }
.v .vwrap .vheader .vinput { border-bottom-color: #223441; }
.v .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.v code, .v pre,.v .vlist .vcard .vhead .vsys { background: #203240; }
.v .vlist .vcard .vcontent.expand:before { background: linear-gradient(180deg,hsla(206,33%,19%,0),hsla(206,33%,19%,.9)); }
.v .vlist .vcard .vcontent.expand:after { background: hsla(206,33%,19%,.9); }</style><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@1.4.0/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'faWhCflfYAf4DSBY6i31K1r7-gzGzoHsz',
  appKey:'CKuisu7G1GD4VMH48mNdbWjX',
  placeholder:'无需注册，随便说的什么吧，留下邮箱的话，被回复会有邮件提醒哦ヾ(o◕∀◕)ﾉヾ',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script></section><footer><p>Copyright © 2016 - 2021 <a href="/." rel="nofollow">YOYO</a><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"></body></html>