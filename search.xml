<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【算法笔记】目标检测之YOLO系列v1v2v3</title>
      <link href="/post/79989519.html"/>
      <url>/post/79989519.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/images/v2-181f533e731b216a98b238af42ffbb2b_r.jpg" alt="picture 3"></p><h1 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li>1、YOLO是一阶段的方法，速度比RCNN系列更快。</li><li>2、YOLO在CNN中计算了图像中的全部特征，而RCNN在分类时系列只看到了局部特征。即YOLO使用全图作为 Context 信息，背景错误（把背景错认为物体）比较少。</li></ul><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ul><li>1、<strong>Resize</strong>成448*448，图片分割得到7*7网格(cell)</li><li>2、<strong>CNN提取特征和预测</strong>：卷积提取特征。全链接部分负责预测。</li><li>3、过滤bbox（通过<strong>nms</strong>）</li></ul><h2 id="输入与输出"><a href="#输入与输出" class="headerlink" title="输入与输出"></a>输入与输出</h2><p>图片缩放到448*448的大小，因为网络结构最后欧全连接层</p><p><img src="/images/bc031ac7178ce4904c91c4e0121970ca983948b9afc567d2a41987312e609166.png" alt="picture 3">  </p><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>YOLO将输入图像分成SxS（S=7）个格子，每个格子负责检测‘落入’该格子的物体。若某个物体的中心位置的坐标落入到某个格子，那么这个格子就负责检测出这个物体。</p><p><img src="/images/79d94534d4eaf24e08b60215b7facc8c52a5ab1e125433eb640ad5ca57f57699.png" alt="picture 1">  </p><h3 id="bounding-box"><a href="#bounding-box" class="headerlink" title="bounding box"></a>bounding box</h3><p>为每个格子预先设置B个bounding box（B=2）。则每个格子的输出要包含B个bounding box的信息，包括x,y,w,h,和confidence。<br>其中x,y代表在该格子内的相对偏移，取值[0,1]。w,h取值可以大于1。</p><p>bounding box的置信度Label计算方式为:</p><script type="math/tex; mode=display">\text { Confidence }=\operatorname{Pr}(\text { Object }) * I O U_{\text {pred }}^{\text {truth }}</script><ul><li>$\operatorname{Pr}(\text { Object })$代表该bounding box中是否包含物体。若包含物体，则P(object) = 1；否则P(object) = 0。 </li><li>$I O U_{\text {pred }}^{\text {truth }}$为预测的bounding box与真实bounding box的接近程度。</li><li>Label中的IOU是在训练阶段通过ground truth计算的。在测试时无法计算，也无需计算。</li></ul><h3 id="输出结构"><a href="#输出结构" class="headerlink" title="输出结构"></a>输出结构</h3><p>YOLO原来需要检测图片中的20中物体，所以要进行20类分类，输出结构如图所示：<br><img src="/images/2f7a00b57168be8545ab1da6a13d3b2ee566b5e3642bda49797b33a9b59df232.png" alt="picture 2">  </p><p>所以输出是一个 7*7*30 的张量</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>YOLO检测网络包括24个卷积层和2个全连接层，如下图所示。<br><img src="/images/b4c405315fee64b85cc8e6dfef6db46293153885c5be3b2af7998ad32b75f5a7.png" alt="picture 7">  </p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/images/764b6b5c72f098d84142127ca2cb0d3ff9758e1568ea4b0d3236c52b8cce6e86.png" alt="picture 8">  </p><p>最上部分为坐标误差，中间部分为IOU误差，最后部分为分类误差</p><p>其中w与h需要先开根号再使用均方误差，原因如下:<br>对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏一点更不能忍受。在下图中small bbox的横轴值较小，发生偏移时，反应到y轴上的loss（下图绿色）比big box(下图红色)要大。<br><img src="/images/5e6f1342a0383c8904deeca99f3cf68a0fde7299592ec518f5509026c1fc04f3.png" alt="picture 9">  </p><h1 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h1><h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/images/ab0f956dee2cd91570330f981a0f1d555e4f3df25f9d3d9f015676b50729b7e9.png" alt="picture 12"> </p><h2 id="一系列改进"><a href="#一系列改进" class="headerlink" title="一系列改进"></a>一系列改进</h2><h3 id="1、引入BN"><a href="#1、引入BN" class="headerlink" title="1、引入BN"></a>1、引入BN</h3><p>在每一个卷积层后添加batch normalization，极大的改善了收敛速度同时减少了对其它regularization方法的依赖（舍弃了dropout依然没有过拟合），使得mAP获得了2%的提升。</p><h3 id="2、高分辨率预训练"><a href="#2、高分辨率预训练" class="headerlink" title="2、高分辨率预训练"></a>2、高分辨率预训练</h3><p>YOLOv1在训练时先使用224x224的图片输入来预训练自己的特征提取网络，再使用448x448的数据集进行finetune。这说明网络需要重新学习识别大尺度（448）的图片以及学习进行其上的目标检测工作。</p><p>YOLOv2首先修改预训练分类网络的分辨率为448*448，在ImageNet数据集上训练10轮（10 epochs）。这个过程让网络有足够的时间调整filter去适应高分辨率的输入。然后fine tune为检测网络。mAP获得了4%的提升。</p><h3 id="3、使用anchor-box"><a href="#3、使用anchor-box" class="headerlink" title="3、使用anchor box"></a>3、使用anchor box</h3><p>YOLO(v1)使用全连接层数据进行bounding box预测（要把1470<em>1的全链接层reshape为7</em>7*30的最终特征），这会丢失较多的空间信息定位不准。</p><ul><li>移除全连接层</li><li>去掉最后的池化层确保输出的卷积特征图有更高的分辨率。</li><li>缩减网络，让图片输入分辨率为416 * 416，目的是让后面产生的卷积特征图宽高都为奇数</li><li>使用卷积层降采样（factor 为32），最终得到13 * 13的卷积特征图（416/32=13）。</li><li>把预测类别的机制从空间位置(cell)中解耦，由9个anchor box同时预测类别和坐标。</li></ul><p><img src="/images/2d133c1dd9c168c75cb4579c3604fb2a52fa7f5d7a16a955147a5eef36db6793.png" alt="picture 10">  </p><p>共会预测13 <em> 13 </em> 9 = 1521个boxes，而之前的网络仅仅预测7 <em> 7 </em> 2 = 98个boxes。</p><h3 id="4、Kmeans寻找预设anchors形状"><a href="#4、Kmeans寻找预设anchors形状" class="headerlink" title="4、Kmeans寻找预设anchors形状"></a>4、Kmeans寻找预设anchors形状</h3><p>选择了更好的、更有代表性的先验boxes维度，那么网络就应该更容易学到准确的预测位置。<br>使用K-means聚类方法，通过对数据集中的ground truth box做聚类，找到ground truth box的统计规律。<br>若使用欧式距离，则大boxes比小boxes产生更多error。使用以下距离：</p><script type="math/tex; mode=display">d(\text { box }, \text { centroid })=1-I O U(\text { box, centroid })</script><p>最终选择k=5。</p><h3 id="5、预测相对位置"><a href="#5、预测相对位置" class="headerlink" title="5、预测相对位置"></a>5、预测相对位置</h3><p>由于借鉴了Faster-RCNN的anchor，但使用Faster-RCNN的位置回归计算方式不容易收敛，因为$x=\left(t_{x} <em> w_{a}\right)+x_{a}, y=\left(t_{y} </em> h_{a}\right)+y_{a}$,这个公式没有任何限制，无论在什么位置进行预测，任何anchor boxes可以在图像中任意一点。<br>所以要预测相对位置，网络在特征图（13 *13 ）的每个cell上预测5个bounding boxes，每一个bounding box预测5个坐标值：tx，ty，tw，th，to。如果这个cell距离图像左上角的边距为（cx，cy）以及该cell对应的box的长和宽分别为（pw，ph），那么对应的box为：</p><script type="math/tex; mode=display">\begin{aligned}b_{x} &=\sigma\left(t_{x}\right)+c_{x} \\b_{y} &=\sigma\left(t_{y}\right)+c_{y} \\b_{w} &=p_{w} e^{t_{w}} \\b_{h} &=p_{h} e^{t_{h}} \\\operatorname{Pr}(\text { object }) * I O U(b, \text { object }) &=\sigma\left(t_{o}\right)\end{aligned}</script><p><img src="/images/919ee5fda3ab9c5bc34103b3da43187009c3ec682ba64f48ae689bc421b0a927.png" alt="picture 11">  </p><h3 id="6、细粒度特征"><a href="#6、细粒度特征" class="headerlink" title="6、细粒度特征"></a>6、细粒度特征</h3><p>在不同层次的特征图上产生区域建议以获得多尺度的适应性。<br>YOLO v2使用passthrough layer，将倒数第二层特征图（26<em>26）叠加相邻特征到不同通道，把26 </em> 26 <em> 512的特征图叠加成13 </em> 13 * 2048的特征图，与原生的深层特征图相连接。</p><p><img src="/images/4d20d94fb2100a6cd77c587828a47879c5c2d1ff1da9b4452efc8a01d0a5acd8.png" alt="picture 2">  </p><h3 id="7、多尺度训练"><a href="#7、多尺度训练" class="headerlink" title="7、多尺度训练"></a>7、多尺度训练</h3><p>由于网络取消了全连接层，所以可以使用多种分辨率的输入图片进行训练。</p><h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/images/591302b840137bf0e9d748bc7ee6a0460f3cfc60168c15a41be1b8c87f6cea61.png" alt="picture 3">  </p><ul><li>第一行计算的是当前anchor box内无物体时，希望预测结果为0。（也就是背景置信度误差）</li><li>第二行为预测结果与预设anchor box形状的误差，让网络从启动后快速学到anchor的形状的过程，只在前12800次迭代正起作用。</li><li>第三行为<strong>坐标损失</strong>预测结果与ground truth的误差</li><li>第四行为<strong>置信度损失</strong>，也就是每个anchor box的conferen的损失</li><li>第五行为<strong>分类损失</strong>，均方误差与YOLO v1一样。</li><li>其中第2行需要变为$(2-w_i<em>h_i)</em>(prior_k^r-b_{ijk}^r)$，第三行同理。原因与YOLO v1相同，大目标对wh约束不强，小目标更严格。而YOLO v1中的开根号方式作者认为没用。所以改用这样的形式。</li></ul><h1 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h1><h2 id="网络结构-2"><a href="#网络结构-2" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/images/e4b858710199a4233518d9af0171daa965a3be843878cfba0c9f1eab985e0607.png" alt="picture 13">  </p><p><img src="/images/e084250da679132aa454eccdc115e72d4ec1ed8be4fd7872177cab7d3c764b25.png" alt="picture 1">  </p><p>此结构主要由75个卷基层构成，没有池化层和全连接层，张量的尺寸变换是通过改变卷积核的步长来实现的。<br>yolo_v2中对于前向过程中张量尺寸变换，都是通过 最大池化来进行，一共有5次。而v3是通过卷积核 增大步长来进行，也是5次。</p><p>网络预测部分类似FPN结构：<br><img src="/images/3b8f4f275b9260527b828cb545088f89f650107954f9150fdc3ecf5940c59823.png" alt="picture 4">  </p><h2 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h2><p>每个输出都为255维向量，规模分别为13，26，52为在不同尺度上进行预测的结果。输出使用3个bounding box，COCO数据集共80类，每个box有（x，y，w，h，conference）信息，所以（80+5）*3=255。</p><p>与YOLOv2一样，YOLOv3也是在feature map上对每个位置进行bbox预测。预测相对当前grid的相对值，分别是(tx,ty,tw,th)。最终的预测bbox为：bx,by,bw,bh，这是在image的bbox。（唯一不同是YOLOv2用5个box）</p><h2 id="替换softmax层：对应多重label分类"><a href="#替换softmax层：对应多重label分类" class="headerlink" title="替换softmax层：对应多重label分类"></a>替换softmax层：对应多重label分类</h2><p>Softmax层被替换为一个1x1的卷积层+logistic激活函数的结构。使用softmax层的时候其实已经假设每个输出仅对应某一个单个的class，但是在某些class存在重叠情中，使用softmax就不能使网络对数据进行很好的拟合。</p><h2 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/images/64bd2aeef1de28784d899d5b5432040612023cf29ee4dba87d9764e3108264d5.png" alt="picture 5">  </p><ul><li>上面两行为bounding box预测误差，与YOLO v2相同。</li><li>第三行第四行为bounding box 的conference的二元交叉熵误差，第三行为有物体情况，第四行为无物体情况。由于图片中大部分区域无物体，所以无物体情况乘衰减系数$\lambda_{noobj}$，比如0.5</li><li>第五行为分类误差，使用二元交叉熵。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/46691043" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46691043</a><br><a href="https://zhuanlan.zhihu.com/p/24916786" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24916786</a><br><a href="https://zhuanlan.zhihu.com/p/25236464/" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25236464/</a><br><a href="https://blog.csdn.net/c20081052/article/details/80236015" target="_blank" rel="noopener">https://blog.csdn.net/c20081052/article/details/80236015</a><br><a href="https://zhuanlan.zhihu.com/p/25167153" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25167153</a><br><a href="https://blog.csdn.net/leviopku/article/details/82660381" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/82660381</a><br><a href="https://zhuanlan.zhihu.com/p/40332004" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40332004</a><br><a href="https://blog.csdn.net/just_sort/article/details/103232484" target="_blank" rel="noopener">https://blog.csdn.net/just_sort/article/details/103232484</a><br><a href="https://zhuanlan.zhihu.com/p/142408168" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/142408168</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【算法笔记】目标检测之Fastst RCNN与Faster RCNN</title>
      <link href="/post/205a2488.html"/>
      <url>/post/205a2488.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/images/12.jpg" alt="1">  </p><h1 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h1><ul><li>RCNN对每个region都进行CNN提取特征，由于region重叠部分较多，使CNN对相同区域提取多次，耗费时间。</li><li>并且RCNN提取region特征之前需要resize，破坏图中内容的结构，影响效果。</li></ul><p>所以Fast R-CNN诞生了。</p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p><img src="/images/4efe5f0398d1155f780581e60fcb227ce4af794baf8a9a6efac1f947acca20e4.png" alt="picture 3"><br><img src="/images/91920bcf17c49974aa9f64fe1f934c79e0dac19a53f5e69c1f7cf39c2f7f28ae.png" alt="picture 4">  </p><ul><li><p>如图，Fast R-CNN将整张图片resize送入神经网络（RCNN是对每个region进行resize），在最后一层再加入候选框信息（这些候选框还是经过Selective Search提取，再将在原图中的位置映射到最后一层特征图上），这样CNN提取特征只需对整张图做一次提取即可。</p></li><li><p>使用ROI pooling layer对最后一层特征图中每个region的对应的区域进行pooling，产生固定大小的输出，再送进FC层。</p></li><li><p>FC层的输出两部分，使用了多任务损失函数(multi-task loss)。</p></li></ul><h2 id="ROI-pooling-layer"><a href="#ROI-pooling-layer" class="headerlink" title="ROI pooling layer"></a>ROI pooling layer</h2><p>作用：对任意大小的输入产生固定的输出</p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ol><li><p>提取特征的网络的最后一个特征图层；</p></li><li><p>一个表示图片中所有ROI的N*5的矩阵，其中N表示ROI（region of interest）的数目。第一列表示图像index，其余四列表示坐标（x,y,h,w）。坐标的参考系不是针对feature map的。其中x，y表示RoI的左上角在整个图片中的坐标。</p></li></ol><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>输出为，与ROI同数量的vector，vector大小为channel<em>w</em>h。channel为最后一层feature map的channel数。w，h为约定好的固定大小的输出。</p><h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><ul><li>1、根据输入image，将ROI坐标映射到feature map对应位置</li><li>将映射后的区域划分为相同大小的区域，区域数量与约定好的输出数量相同。</li><li>对每个区域进行max pooling操作</li></ul><p>具体计算过程如下动图所示（假设输出w，h为2*2）：<br><img src="/images/20181110170648243.gif" alt="picture1"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/images/0a38f343339dc7fcc9fddb0fa3f9913a488af2570eb77489c943bd7adf72bc22.png" alt="picture 5"><br><img src="/images/2e7e9dd3f5a8e2629914de9ea3360339c6a59ca8003535f902ac6dfea6096516.png" alt="picture 6"><br>smooth的图像为：<br><img src="/images/7d07bffc5972ad9013cfd990e8b9bd20dc8bbd67da21c92260947cafae861ac9.png" alt="picture 7">  </p><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>Fast R-CNN与R-CNN一样需要预先Selective Search，该过程速度较慢。<br>所以Faster R-CNN将region提取的过程融入进网络里。</p><h2 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h2><p><img src="/images/0a1d681b4d6d86fc9b1bf4f912f74e7fb5794c550f2df807dc0d419428356993.png" alt="picture 8">  </p><p>从图中可以看出，该网络结构大部分与Fast R-CNN相同，唯一不同的是最后一层卷积的输出feature map输入给了<strong>RPN</strong>网络来生成region位置，并使用生成的位置和最后一层feature map作为ROI pooling layer的输入。</p><h2 id="区域生成网络-（-Region-Proposal-Networks-）"><a href="#区域生成网络-（-Region-Proposal-Networks-）" class="headerlink" title="区域生成网络 （ Region Proposal Networks ）"></a>区域生成网络 （ Region Proposal Networks ）</h2><p>RPN可以帮我们找出可能包含物体的那些区域</p><p>RPN使用固定大小的anchor，这些anchor将会均匀地放置在整个原始图像中。不同于原来我们要检测物体在哪里，我们现在利用anchor将问题转换为另外两部分：</p><ul><li>某个框内是否含有物体</li><li>某个框是否框的准，如果框的不准我们要如何调整框</li></ul><h3 id="anchors"><a href="#anchors" class="headerlink" title="anchors"></a>anchors</h3><p>如果直接学习物体边框$(x_{min},x_{max},y_{min},y_{max})$是困难的：</p><ul><li>边框数量不定，网络很难输出变长的数据</li><li>物体是不同大小有不同的宽高比，那训练一个效果很好的检测模型将会是非常复杂的</li><li>会存在一些无效的预测，比如$x_{max}&lt;x_{min}$时</li></ul><p>所以RPN使用anchors的方法。RPN在feature map对应的原图位置上，以每个点为中心上使用9个anchors:</p><ul><li>三种面积{128,256,512}</li><li>三种比例{1:1,1:2,2:1}</li></ul><p>如下图所示：<br><img src="/images/eddeed3b1712669742b5754910bebdc3effb65c024501b099a583dcc8077bbf4.png" alt="picture 1">  </p><p>最终将学习每个anchor中是否有东西，以及anchor坐标的偏移（学习偏移比直接学习边框位置简单）。<br>anchor位置：$(x_{center},y_{center},width,height)$</p><p>学习的偏移：$(\Delta x_{center},\Delta y_{center},\Delta width,\Delta height)$</p><p>修正过程：（g为ground-truth，a为anchor）</p><script type="math/tex; mode=display">\begin{aligned}\Delta x_{\text {center }}&=\left(x_{g}-x_{a}\right) \times \text { width }_{a}\\ \Delta y_{\text {center }}&=\left(y_{g}-y_{a}\right) \times \text { height }_{a} \\\Delta \text { width }&=\log \left(\text { width }_{g} / \text { width }_{a}\right)\\ \Delta \text { height }&=\log \left(\text { height }_{g} / \text { height }_{a}\right)\end{aligned}</script><h3 id="输入与输出"><a href="#输入与输出" class="headerlink" title="输入与输出"></a>输入与输出</h3><p>我们使用第一部分返回的feature map进行输入，以全卷积的方式实现RPN。我们首先使用一个3*3*512的卷积层，然后使用两个并行的1*1的卷积内核，其中两个并行卷积层的通道数取决于每个点的Anchor数量（k个）。</p><p>如图所示：<br><img src="/images/3104a00f7fd0e1752a4b0b636a321c5eb0ae3009472d43b469c9a07ae334ec8c.png" alt="picture 2">  </p><ul><li>第一部分为分类层（左侧），我们为每个Anchor输出2个预测值：<strong>背景得分以及前景得分</strong>（实际含有物体）。</li><li>第二部分为回归层，输出4个预测值：为anchor的偏移值$(\Delta x_{center},\Delta y_{center},\Delta width,\Delta height)$</li></ul><p>卷积过程如下图所示：<br><img src="/images/d16ffcec8f681ee0840b1367a0c559ee6ead04f6258f01c44ab24d9964e17d11.png" alt="picture 3">  </p><ul><li>第一个3<em>3卷积相当于将每个3\</em>3的滑动窗口转化为一个1维向量代表该滑动窗口内的特征。该滑动窗口的感受野包括了9个以该点位中心的anchor的面积。该卷积需要1*1 padding，即边缘部分的滑动窗口也需要计算。</li><li>之后的两个1*1卷积相当于FC层，相当于将每个滑动窗口的特征向量进行计算，得到该滑动窗口内的9个anchor的输出值。</li><li>两次卷积的输出的w,h不变，代表该这些滑动窗口的计算结果。</li></ul><h3 id="RPN网络的训练以及损失函数"><a href="#RPN网络的训练以及损失函数" class="headerlink" title="RPN网络的训练以及损失函数"></a>RPN网络的训练以及损失函数</h3><h4 id="监督标签"><a href="#监督标签" class="headerlink" title="监督标签"></a>监督标签</h4><p>与ground-truth的IoU大于0.7的Anchor视为前景（如果没有就找一个与ground-truth的IoU最大的anchor），而与真实物体IoU小于0.3的视为背景。而IoU处于两个阈值中间的则忽略。</p><script type="math/tex; mode=display">p *=\left\{\begin{array}{ll}1, & I o U>0.7 \text { or max IoU for ground }-\text { truth } \\0, & I o U<0.3 \\\text { ignore, } & 0.3<=I o U<=0.7\end{array}\right.</script><ul><li>每个ground-turth可能有多个anchor对应</li><li>打上正标签的anchor，将对应的groud-truth位置作为回归学习的目标</li></ul><h4 id="平衡正负样本"><a href="#平衡正负样本" class="headerlink" title="平衡正负样本"></a>平衡正负样本</h4><p>由于图中大部分区域都是背景，所以负标签较多。则每次使用mini-batch采样训练（一般为256）。每次采样128正样本anchor，并采样同数量负样本。（若正样本不足128，则采样同数量负样本）。</p><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><ul><li>对于分类损失，通过二分类交叉熵进行计算</li><li>对应回归损失，只会针对那些正样本进行回归，具体来说就是对偏移进行回归$(\Delta x_{center},\Delta y_{center},\Delta width,\Delta height)$。损失函数使用<strong>Smooth L1损失</strong>。</li></ul><h3 id="RPN的输出处理"><a href="#RPN的输出处理" class="headerlink" title="RPN的输出处理"></a>RPN的输出处理</h3><ul><li>取所有输出为正的region的得分前M个region作为输出。</li><li>clip限定超出图像边界的前景anchor作为图像边界<br><img src="/images/29c3b60a0aea9bfae774351aabf49e474e72db2e8e6b267ab3968728b7715f8b.png" alt="picture 4">  </li><li>忽略掉长或者宽太小的建议框</li><li>进行NMS</li><li>最后重新再按得分排序，取得分前N个框</li></ul><h2 id="Faster-RCNN整体训练过程"><a href="#Faster-RCNN整体训练过程" class="headerlink" title="Faster-RCNN整体训练过程"></a>Faster-RCNN整体训练过程</h2><p><img src="/images/2f9231a620f472cf29f177a9a645543758b7a40c0f22053aa0992c12a6528972.png" alt="picture 7">  </p><p>第一步：用ImageNet模型初始化，独立训练一个RPN网络；<br>第二步：仍然用ImageNet模型初始化，但是使用上一步RPN网络产生的proposal作为输入，训练一个Fast-RCNN网络，至此，两个网络每一层的参数完全不共享；<br>第三步：使用第二步的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的learning rate设置为0，也就是不更新，仅仅更新RPN特有的那些网络层，重新训练，此时，两个网络已经共享了所有公共的卷积层；<br>第四步：仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个unified network，继续训练，fine tune Fast-RCNN特有的网络层，此时，该网络已经实现我们设想的目标，即网络内部预测proposal并实现检测的功能。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/24780395" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24780395</a><br><a href="https://zhuanlan.zhihu.com/p/24916624" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24916624</a><br><a href="https://zhuanlan.zhihu.com/p/123962549" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/123962549</a><br><a href="https://blog.csdn.net/w437684664/article/details/104238521" target="_blank" rel="noopener">https://blog.csdn.net/w437684664/article/details/104238521</a><br><a href="https://www.cnblogs.com/jiangnanyanyuchen/p/9433791.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangnanyanyuchen/p/9433791.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【算法笔记】目标检测之RCNN</title>
      <link href="/post/664697e9.html"/>
      <url>/post/664697e9.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><p><img src="/images/ee586c0405805558105a95efc3fdadd586744ba99895b06eb6a2de9d88db458d.png" alt="picture 2">  </p><p>1、输入测试图片<br>2、利用<strong>selective search</strong>算法从图像中提取2k左右个region proposals（备选区域）<br>3、将每一个region proposals缩放成227x227的大小并输入CNN网络，在最后一层全连接输出图像特征（特征提取）<br>4、把每个region proposal的CNN特征输入到SVM进行分类处理<br>5、对SVM分类好的region proposal进行边框回归处理，使得预测框和真实框更加吻合（边框回归）</p><hr><h1 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a>Selective Search</h1><pre><code>--------------------------------利用切分方法得到候选的区域集合R = {r1,r2,…,rn}初始化相似集合S = ϕforeach 遍历邻居区域对(ri,rj) do    计算相似度s(ri,rj)    S = S  ∪ s(ri,rj)while S not=ϕ do    从S中得到最大的相似度s(ri,rj)=max(S)    合并对应的区域rt = ri ∪ rj    移除ri对应的所有相似度：S = S\s(ri,r*)    移除rj对应的所有相似度：S = S\s(r*,rj)    计算rt对应的相似度集合St    S = S ∪ St    R = R ∪ rt</code></pre><p>其中第一步生成候选区域R所需算法：<br>基于图的图像分割（Graph-Based Image Segmentation）<br><a href="https://blog.csdn.net/guoyunfei20/article/details/78727972" target="_blank" rel="noopener">https://blog.csdn.net/guoyunfei20/article/details/78727972</a></p><h2 id="评价相似度"><a href="#评价相似度" class="headerlink" title="评价相似度"></a>评价相似度</h2><p>两块区域为$r_i,r_j$</p><p><strong>1、颜色相似度</strong><br>使用L1-norm归一化获取图像每个颜色通道的25 bins的直方图，这样每个区域都可以得到一个75维的向量$\{c_i^1,\dots, c_i^n \}$，区域之间颜色相似度通过下面的公式计算：</p><script type="math/tex; mode=display">s_{\text {colour }}\left(r_{i}, r_{j}\right)=\sum_{k=1}^{n} \min \left(c_{i}^{k}, c_{j}^{k}\right)</script><p>每一个颜色通道的直方图累加和为1.0，三个通道的累加和就为3.0，如果区域ci和区域cj直方图完全一样，则此时颜色相似度最大为3.0，如果不一样，由于累加取两个区域bin的最小值进行累加，当直方图差距越大，累加的和就会越小，即颜色相似度越小。<br>合并后新相似度为：</p><script type="math/tex; mode=display">C_{t}=\frac{\operatorname{size}\left(r_{i}\right) \times C_{i}+\operatorname{size}\left(r_{j}\right) \times C_{j}}{\operatorname{size}\left(r_{i}\right)+\operatorname{size}\left(\mathrm{r}_{\mathrm{j}}\right)}</script><p><strong>2、纹理相似度</strong><br>每个颜色通道的8个不同方向计算方差σ=1的高斯微分（Gaussian Derivative），使用L1-norm归一化获取图像每个颜色通道的每个方向的10 bins的直方图，这样就可以获取到一个240（10x8x3）维的向量$T_i=\{t_i^1\dots t_i^n\}$，区域之间纹理相似度计算方式和颜色相似度计算方式类似，合并之后新区域的纹理特征计算方式和颜色特征计算相同：</p><script type="math/tex; mode=display">s_{\text {texture }}\left(r_{i}, r_{j}\right)=\sum_{k=1}^{n} \min \left(t_{i}^{k}, t_{j}^{k}\right)</script><p><strong>3、优先合并小的区域（小的区域权重更大）</strong><br>否则，容易使得合并后的区域不断吞并周围的区域。</p><script type="math/tex; mode=display">s_{\text {size }}\left(r_{i}, r_{j}\right)=1-\frac{\operatorname{size}\left(r_{i}\right)+\operatorname{size}\left(\mathrm{r}_{\mathrm{j}}\right)}{\operatorname{size}(image)}</script><p><strong>4、区域的合适度距离</strong><br>这里定义区域的合适度距离主要是为了衡量两个区域是否更加“吻合”，其指标是合并后的区域的Bounding Box（能够框住区域的最小矩形BBij）越小，其吻合度越高，即相似度越接近1。</p><script type="math/tex; mode=display">S_{fill}\left(r_{i}, r_{j}\right)=1-\frac{\operatorname{size}\left(B B_{i j}\right)-\operatorname{size}\left(r_{i}\right)-\operatorname{size}\left(r_{i}\right)}{\operatorname{size}(i m)}</script><p><strong>合并</strong></p><script type="math/tex; mode=display">\begin{aligned}s\left(r_{i}, r_{j}\right)=& a_{1} s_{\text {colour }}\left(r_{i}, r_{j}\right)+a_{2} s_{\text {texture }}\left(r_{i}, r_{j}\right)+\\& a_{3} s_{\text {size }}\left(r_{i}, r_{j}\right)+a_{4} s_{\text {fill }}\left(r_{i}, r_{j}\right) \\\text { 其中 } a_{i} \in\{0,1\} &\end{aligned}</script><hr><h1 id="CNN结构与类别预测"><a href="#CNN结构与类别预测" class="headerlink" title="CNN结构与类别预测"></a>CNN结构与类别预测</h1><p><strong>Alexnet</strong></p><p><img src="/images/7dd93086f3d68bfa85b8c11ee0003f83d48c8aa0c100f2701a79913d77ba4298.png" alt="picture 3">  </p><ul><li>因为R-CNN的网络结构中存在有全连接层（fc）,需要输入图像的尺寸保持一致，所以必须resize成227*227。</li><li>一个region proposal经过CNN网络输出4096维的特征</li><li>对每一类使用SVM进行二分类，判断是否属于此类。</li></ul><hr><h1 id="边框回归"><a href="#边框回归" class="headerlink" title="边框回归"></a>边框回归</h1><p>虽然已经使用了selective search来最大限度地提取目标的候选框，但有些候选框与真实框依旧有很大差距，因此使用一个线性方程来实现位置的精确定位。<br><img src="/images/7d6b66459999be00434deece4ae1dadb880b046dd541e31d22652f74658a3a2e.png" alt="picture 4"><br>如上图，黄色框口P表示建议框Region Proposal，绿色窗口G表示实际框Ground Truth，红色窗口表示Region Proposal进行回归后的预测窗口，现在的目标是找到P到$\hat{G}$的线性变换，使得与G越相近。<br>若$G=\left(G_{x}, G_{y}, G_{w}, G_{h}\right)$,四个元素的位移为：$d_{x}(P), d_{y}(P), d_{w}(P), d_{h}(P)$,则建立方程组：</p><script type="math/tex; mode=display">\begin{array}{l}\hat{G}_{x}=P_{w} d_{x}(P)+P_{x} \\\hat{G}_{y}=P_{h} d_{y}(P)+P_{y} \\\hat{G}_{w}=P_{w} \exp \left(d_{w}(P)\right) \\\hat{G}_{h}=P_{h} \exp \left(d_{h}(P)\right)\end{array}</script><p>每一个 $d_<em>(P)$都是一个AlexNet CNN网络Pool5层特征$\phi _5(P)$的线性函数$d_{</em>}(P)=w_{*}^{T} \phi_{5}(P)$，其中$w$为需要学习的参数。则损失函数为：</p><script type="math/tex; mode=display">\text { Loss }=\operatorname{argmin} \sum_{i=0}^{N}\left(t_{*}^{i}-\hat{w}_{*}^{T} \phi_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{w}_{*}\right\|^{2}</script><p>其中：</p><script type="math/tex; mode=display">\begin{aligned}t_{y} &=\left(G_{y}-P_{y}\right) / P_{h} \\t_{w} &=\log \left(G_{w} / P_{w}\right) \\t_{h} &=\log \left(G_{h} / P_{h}\right)\end{aligned}</script><hr><h1 id="NMS-非极大抑制"><a href="#NMS-非极大抑制" class="headerlink" title="NMS(非极大抑制)"></a>NMS(非极大抑制)</h1><p>假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大的概率 分别为A、B、C、D、E、F。</p><p>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;</p><p>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</p><p>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</p><p>就这样一直重复，找到所有被保留下来的矩形框。</p><p>代码：</p><pre><code class="lang-python">def NMS(dects,threshhold):    &quot;&quot;&quot;    detcs:二维数组(n_samples,5)    5列：x1,y1,x2,y2,score    threshhold: IOU阈值    &quot;&quot;&quot;    x1=dects[:,0]    y1=dects[:,1]    x2=dects[:,2]    y2=dects[:,3]    score=dects[:,4]    ndects=dects.shape[0]#box的数量    area=(x2-x1+1)*(y2-y1+1)    order=score.argsort()[::-1] #score从大到小排列的indexs,一维数组    keep=[] #保存符合条件的index    suppressed=np.array([0]*ndects) #初始化为0，若大于threshhold,变为1，表示被抑制    for _i in range(ndects):        i=order[_i]  #从得分最高的开始遍历        if suppressed[i]==1:            continue        keep.append(i)         for _j in range(i+1,ndects):            j=order[_j]            if suppressed[j]==1: #若已经被抑制，跳过                continue            xx1=np.max(x1[i],x1[j])#求两个box的交集面积interface            yy1=np.max(y1[i],y1j])            xx2=np.min(x2[i],x2[j])            yy2=np.min(y2[i],y2[j])            w=np.max(0,xx2-xx1+1)            h=np.max(0,yy2-yy1+1)            interface=w*h            overlap=interface/(area[i]+area[j]-interface) #计算IOU（交/并）            if overlap&gt;=threshhold:#IOU若大于阈值，则抑制                suppressed[j]=1    return keep</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/23006190" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/23006190</a><br><a href="https://zhuanlan.zhihu.com/p/53829435" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/53829435</a><br><a href="https://blog.csdn.net/qq_43243022/article/details/88895057" target="_blank" rel="noopener">https://blog.csdn.net/qq_43243022/article/details/88895057</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【杂谈】2020届北航计算机，清华软件保研经验</title>
      <link href="/post/39d6c6d8.html"/>
      <url>/post/39d6c6d8.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/img/by.jpg" alt="封面"></p><p>本人定位：</p><ul><li>学校：吉林大学</li><li>排名：由于学校排名方式有变化，所以夏令营时期的排名是30/351，预推免时期的排名是19/351</li><li>比赛：参加了各种水比赛（比如天梯赛，ccf ccsp，数学建模等），得的奖也都是各种水奖，不值一提。</li><li>项目：学校的小型创新项目（飞行棋），大创项目（人脸表情识别，运动物体检测），以及各种课设。</li><li>科研：关于生物信息大数据的SCI一篇，共同一作（名字排名是第二个）</li><li>其他：六级502。</li><li>夏令营：南大lamda（被拒），北大信工（被拒），北航计算机（优营）</li><li>预推免：清华软件（通过），北大软微（通过初审，没去）</li></ul><hr><p>大三下学期，学习有要求只能开三份成绩排名的证明，所以无法采取海投策略，于是只投了三个。</p><p>由于南大lamda是5.20截止，我在5.19突然决定投一个试试，所以很匆忙，材料写的乱七八糟，甚至连实验室的名称都写错了。人家的名字是lamda，是Learning And Mining from DatA的意思，我还以为是LAMBDA表达式的意思，结果个人陈述中的实验室名字打错了。。。。。果然收到了南大lamda拒信，被拒是必然的，且不说我连人家名字都打错了，我自身的水平也不够，所以并不是特别沮丧。不过这封拒信写的还挺好，让人即使被拒也挺开心的。。<img src="/img/hj.png" alt="滑稽"></p><p><div align="center">  <img src="/img/lamda.jpg"></div><br>之后准备北大信工的夏令营材料，两封副教授以上的推荐信，个人陈述，以及各种表格和复印件，之后还拜托了我的本科导师帮忙联系了北大信工的一位老师，得到了标准答复：夏令营见。结果最后没入营，凉凉。</p><p>北航计算机如果不报直博的话就不需要推荐信，各种材料也不麻烦，个人陈述的那一项我直接把简历发上去了，也没有提前联系老师（事后比较后悔，尽量能提前联系就提前联系吧）。成功入营，一共入了598人，惊了。</p><hr><h1 id="北航夏令营之行"><a href="#北航夏令营之行" class="headerlink" title="北航夏令营之行"></a>北航夏令营之行</h1><h3 id="7-11"><a href="#7-11" class="headerlink" title="7.11"></a>7.11</h3><p>上午报道交材料交csp成绩来抵机试（本人315分，当次排名2.28%，历史排名3.9%），之后各个实验室的老师上台宣讲，下午机试不用参加所以回宾馆看片+复习。晚上面试名单出来了，只剩下284人，太惨烈了。面试分两天，我是第一天，是我们组第二个面试的，当天晚上非常紧张，一直复习到凌晨三点才睡觉。</p><h3 id="7-12"><a href="#7-12" class="headerlink" title="7.12"></a>7.12</h3><p>八点到面试门口，第一个进去了，里面传来了阵阵笑声，不知问了啥会引发大笑。不过我在外面可是紧张的不得了，拿着笔记来回走。要是之前好好复习的话就不会有那么多事了。八点半左右，轮到我了，进去发了简历。先抽一道政治题，如何建设网络强国，这题我在保研经验上看到过，没想到我自己也抽到了，瞎几把答，要加强网络教育巴拉巴拉，前言不搭后语，不过老师好像也不怎么在意这部分。之后英语测试，先读一段文章，再翻译，是软件工程方面的。我读的简直是灾难，加上紧张，读完一遍完全不知道读的是什么，声音都有点颤抖，之后翻译也胡编乱造，有个词实在是编不出来了就过了。之后开始了数学与专业面试。</p><ul><li><p><font size="4">什么是良序（离散数学）</font><br>&emsp;&emsp;我本科学的离散数学里没提到过良序的概念，后来我才知道。不过老师又问了什么是偏序，才勉强答上。</p><blockquote><p>设集合(S,≤)为一全序集，≤是其偏序关系，若对任意的S的非空子集，在其序下都有最小元素，则称≤为良序关系，(S,≤)为良序集。</p></blockquote></li><li><p><font size="4">A是矩阵，Ax=b什么时候有解（线性代数）</font><br>&emsp;&emsp;其实只要从矩阵A的秩与增广矩阵[A b]的秩的关系上回答就好，我说得好复杂，还解释了一通线性空间的生成。老师追问什么是线性空间，我记得要满足一系列要求的域就是线性空间，然而当时太紧张了，这一系列要求全忘了。。。。</p><blockquote><p>设V是一个非空集合，P是一个域。若：<br>1.在V中定义了一种运算，称为加法，即对V中任意两个元素α与β都按某一法则对应于V内惟一确定的一个元素α+β，称为α与β的和。<br>2.在P与V的元素间定义了一种运算，称为纯量乘法(亦称数量乘法)，即对V中任意元素α和P中任意元素k，都按某一法则对应V内惟一确定的一个元素kα，称为k与α的积。<br>3.加法与纯量乘法满足以下条件：<br>1) α+β=β+α，对任意α，β∈V.<br>2) α+(β+γ)=(α+β)+γ，对任意α，β，γ∈V.<br>3) 存在一个元素0∈V，对一切α∈V有α+0=α，元素0称为V的零元.<br>4) 对任一α∈V，都存在β∈V使α+β=0，β称为α的负元素，记为-α.<br>5) 对P中单位元1，有1α=α(α∈V).<br>6) 对任意k，l∈P，α∈V有(kl)α=k(lα).<br>7) 对任意k，l∈P，α∈V有(k+l)α=kα+lα.<br>8) 对任意k∈P，α，β∈V有k(α+β)=kα+kβ，<br>则称V为域P上的一个线性空间，或向量空间。</p></blockquote></li><li><p><font size="4">在我的论文中，我做了什么贡献。</font><br>&emsp;&emsp;如实回答（写代码，调参实验，服务器搭建等）</p></li><li><p><font size="4">简历提到的一个软件著作权的内容与具体原理（运动检测）</font><br>&emsp;&emsp;如实回答（背景减除法，然后解释了下具体细节）</p></li><li><p><font size="4">用什么数据结构存储一个稀疏矩阵，用这个结构怎样计算矩阵乘法（三元组+十字链表）</font><br>&emsp;&emsp;没想起来十字链表，答得一塌糊涂。</p></li><li><p><font size="4">遇到一个新的自己从未接触过的领域时如何学习。</font><br>&emsp;&emsp;阅读相关论文，看慕课，看博客，看别人实现好的例子，然后自己慢慢设计。问到这种问题时最好配合自己的亲身经历说明。</p></li><li><p><font size="4">当做项目的时候是先根据前人经验做出一个雏形，还是从头自己细细琢磨</font><br>&emsp;&emsp;答：看ddl，近的话就先做出来，交完之后再研究具体原理，远的话就慢慢研究。同样配合自己的亲身经历说明。</p></li></ul><p>结束了，感觉面试很糟糕，问的所有知识点我全没答上来，自己复习的知识点又全没考，高数机器学习一点都没问，感觉会挂了，我都做好啥都没有的准备可，开始玩，当天下午去了颐和园，晚上去了鸟巢水立方，第二天去了长城玩。没有联系老师，也没继续复习薄弱的知识点，因为真的觉得自己应该凉了。。</p><p>7.13晚，官网放出了优秀营员名单，我居然过了woc，一共过了173人，我的分数还贼高281.951/300，排名第九，回头分析一下原因，应该是机试用CCF抵了一个比较高的分数，面试时论文加了分，简历上内容比较多，再加上老师问的我的一些经历，我回答的也比较具体。</p><p><div align="center">  <img src="/img/cj.png"></div><br>得知优营后瞬间高兴地不得了，但明天要找目标导师签字，我却没有联系老师，又瞬间慌得不行。找了一晚上，发了邮件，直到第二天早上也没回。。。。</p><h3 id="7-14"><a href="#7-14" class="headerlink" title="7.14"></a>7.14</h3><p>早上领取表格，昨天联系的老师没回我，办公室也没人，一堆人都在走廊闲逛。等了半天不知如何是好，然后就去找了同实验室的hd老师，到了的时候发现里面正在面试。</p><p>等了一会没人后我才进去，原来其他人早就联系好了，并且九点多一起开了个会，我十点多才过来，这已经属于霸面了。老师看了我的简历，问我机试咋样，我说csp抵了，2.28%，老师笑着说还凑活。又问我有什么优势，我赶紧往我准备好的地方引导，我说我动手能力强，用到的一些算法能理解其原理，比如论文用到了SVM。果然老师又笑了，对这个很感兴趣，我开始狂推SVM，老师中途打断我问了问核函数，问了SVM的优化方式（SMO）。SVM还没推完就不用说了，老师说行了，然后给我介绍了下实验室情况，我有什么问题也可以问，这个实验室是一群老师带学生，每个老师并不独立，就是说不用纠结一定要选某个老师，于是我选择了接受实验室内老师的调剂。</p><p>但要进这个实验室还要经过一轮实验室的面试，过去一看有好多人排队，我没提前联系所以只能最后进去了。中午出去吃了个饭，又回去等着，两点多终于只剩我一个人了，才进去开始面试。然而这个面试不是问学习的，而且唠嗑。老师们都很年轻，整个过程特别欢乐。问了我为什么选择北航，问我有什么爱好，我说我喜欢看动漫，老师让我举三个最喜欢的，我说钢之炼金术师，命运石之门，进击的巨人，有一个老师居然问我钢炼喜欢03还是09版，我说09，老师说那太遗憾了，我说03我也看了。然后还问我逛b站吗，是大会员吗。问了我受到的最大挫折是啥等等。全程特别欢乐，感觉很快就十多分钟就出来了，上午进去的同学有20多分钟。</p><p>出来感觉稳了，跟各个小伙伴分享刚才的面试，简直太欢乐了，跟面试老师聊动漫你敢信。<img src="/img/hj.png" alt="滑稽"> 把表交了就回去了。下午五点，收到了offer，被录取了，接收我的老师就是跟我聊动漫的gyf老师，留在北航培养（有杭州和国安），专硕或学硕还没定，要之后再定。北航之行终于告一段落，北航保底，九推也会信心倍增吧。</p><p>7.15，早上去北航校医院体检，结束后就离开北京了。 之后的暑假，前所未有的放松，8月中旬回到学校，开始新一轮的复习，准备预推免。</p><hr><h1 id="清华软件预推免"><a href="#清华软件预推免" class="headerlink" title="清华软件预推免"></a>清华软件预推免</h1><p>我报名的是专硕，通过初审的有90人左右。</p><h3 id="9-15"><a href="#9-15" class="headerlink" title="9.15"></a>9.15</h3><p>复试第一天，早上去东主楼交材料。由于当时我们的排名还没出，所以我交了一张“无法开具排名证明的证明”。。。。我佛了，老师让我等结束后回学校等排名出来，重新盖一份申请表邮寄过来。之后回去歇了一会下午一点机试。</p><p>机房环境很舒服，键盘鼠标也挺舒服，就是电脑有点卡。三小时三道题，发题纸然后把代码存到固定位置。先看第一页的说明，评测居然用的是vs的release模式，1秒内运行完毕即可，这样应该不会超时，还挺良心，然后开始做题。第一题看完我笑了，质因数分解，快速敲完。第二题看完我又笑了，给出二叉树前序中序遍历结果，求从根节点到所有叶节点的路径中遇到的节点之和等于某个特定的数的路径个数，快速敲完。</p><p>剩下的时间全攻第三题。压缩字符串，读了半天题，本想用动态规划，但是本人没打过acm，oj练得也比较少，咋想都想不出大问题跟子问题有什么关系，于是写了个超级暴力的超大dfs，把所有可能的压缩情况全例举出来一个一个比较，没想到样例居然过了，小规模数据可以过，这样能骗点分了。之后的时间就一直在想办法优化，比如用集合记忆优化，循环外提，提前判断，把sstream改成sprintf，确实优化了一些，但复杂点的样例还是很浪费时间，在release模式下勉强一秒，估计根本得不了几分了。</p><p>唉，如果打过acm的话，机试是有绝对优势的，我还是太菜了。<img src="/img/cry.jpg" alt="哭"></p><h3 id="9-16"><a href="#9-16" class="headerlink" title="9.16"></a>9.16</h3><p>在东主楼面试，看了下名单，我还是在第二位面试，跟北航一样，偷偷瞄了下面试的教室，我靠还有摄像机，我看着摄像机很紧张啊，我甚至连第一个人什么时候进去的都不知道，其他人问了别人我才知道原来已经开始了。</p><p>轮到我了，进去本打算用手机录音，结果手抖没按上，日。给老师发了简历，然后自我介绍一下。于是各个老师轮着问，问了很多： </p><blockquote><ul><li>论文内容，我的工作，</li><li>做运动物体检测系统的原因，难点，我的工作，</li><li>问关于大创的内容，</li><li>问关于飞行棋软件著作权中的伪完全化博弈信息是什么意思（不要乱起高端的名字）</li></ul></blockquote><p>到此我还答得不错，之后突然来了个英文问题，把我问蒙了，大意应该是让我详细解释一下神经网络，我艹我没准备这个，我尝试了半天没说出来，最后放弃了，老师问我六级考口语了吗我说没有，太尴尬了。</p><p>然后问我为啥C++73分，我说的73分的是C#，选修课，平时分40分，而我缺席了几次，我的C++91分。之后老师问了数据库，什么是控制设计和逻辑设计，什么是表空间，数据库应该怎样设计，我只简单说了说数据库设计过程，其他两个概念不会。（为什么我完全没听过这些概念啊，看来我们学的数据库是阉割版的。。）老师说你数据库90分学的挺好啊，怎么答不上来呢。估计老师也感到尴尬，说我们还是回到项目上来吧。接下来又问了一些问题： </p><blockquote><ul><li>什么是大创，这个优秀结题的含金量如何，</li><li>简单问了下我简历上提到的视频制作，</li><li>我想做什么方向，</li><li>除了动手能力还有什么优势，</li><li>最后问了我觉得机试怎么样，</li></ul></blockquote><p>这些回答的也挺好，最后的问题很关键，我们所有人都是不知道机试成绩的，老师手里有成绩，但不会告诉你。如果对自己答题的情况很了解就会加分。我如实说我感觉前两题很简单（此时一个老师突然抬头看我），第三题写了个很暴力的算法应该会超时，老师问我为什么没能想出更好的解决办法，问我学没学过算法，我说学过，但没接受过acm培训，都是自己练习的。就结束了，全程感觉很糟糕，英语没回答上来，专业课（数据库）也没回答上来，感觉没啥希望。</p><p>清华之旅就这样结束了总的来说，我这次是比较失败的，没能展示出最好的自己，要是能复习一下数据库的话，要是能做准备准备英语的话。。。唉北航当初也是这样，幸好过了，没过的话我现在就真的无学可上了。</p><p>回学校后，准备把清华没填好的表重新盖章邮寄，跑来跑去一个表盖了五个章。。下午刚刚顺丰邮寄完毕，就收到一封邮件，是清软offer，居然过了。。。卧槽我过了，各种激动，还跟教务处老师确定了一下后续流程，才终于放下心来。 </p><p>后来我又收到了北大软微的初审通过邮件，不过我已经决定去清华了，就回了一封邮件放弃参加。9月28日凌晨填报系统，9月29日下午四点左右，整个流程走完，保研结束了。</p><hr><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>要问保研最重要的是什么，那必须成绩排名是No.1。各个学校的前几名几乎都是横扫各大学校的夏令营，当时各个学校入营名单出来时候，我们都在说“怎么又是这些人啊”。比较流行的一个说法就是，论文与竞赛只能锦上添花，不能雪中送炭，所以学习成绩是最重要的，当然在保证成绩的情况下，其他经历当然是越多越好。当然如果你各种大奖拿到手软，SCI随便发，这种巨佬除外。</p><p>机试也很重要，练习机试可以用leetcode，洛谷，pta等，Poj我感觉太难了（菜鸡如我）。大模拟类的题可以看ccf csp认证历次的第三题。</p><p>然后在准备材料的时候有几种要认真准备：</p><ol><li>简历内容一定要好好斟酌，要尽可能把低端的东西写得高大上一点，并且要完全理解你做的东西是什么，做它的目的是什么，用到了什么知识/语言/框架/算法等，各种细节全都要非常了解，并且还要了解你用到的东西的相关扩展知识。</li><li>推荐信，我一般是把推荐信自己写好，然后发给老师看一下，有的老师会提出一些修改意见，之后老师同意就可以签字了。在写的时候一定要结合实际经历，而不能全篇都是概括性的评价，比如“该同学学习优秀，勤奋上进”之类的，主要应该写“在我的指导下，该同学完成了……，取得了……的成绩”等，内容越详细越好。</li><li>个人陈述，我是按照从大一到大三的时间顺序写的，在什么时候做了什么，由此引发的我某某知识的兴趣，于是怎样学习等等。除学习之外的内容也可以写进去，比如参加了学校的某活动，比如学生会工作等。最后再展望一下研究生的生活，注意内容最好要提到目标院校，否则对方老师可能会认为你只写了一份然后海投，没有诚意。（虽然我感觉老师根本不看这些材料）</li></ol><p>以上就是我的没什么卵用的保研经验，希望能帮助看到的同学。</p><p>保研终于结束了，最难的一道坎算是完全跨过去了。记得清软初审结果还没出的时候，我就心慌的到处看有没有最新消息，那时能够通过初审真的很幸运，因为我的排名实在是太低了，应该是排在我前面的人没有报清软的，才让我捡了个漏。。。如今，我已经把我手里的一副烂牌打出了最好的效果，这一切真的跟做梦一样。收到邮件offer之后，我还盯着邮件掐自己的脸，幸好我没有醒过来。我的运气真的很好，不过我不能总寄希望于运气，还是得提升自己的水平才行。我对研究生生活有几分期待，也有几分恐惧，希望我能将恐惧的心情不断减少吧，当自己成为真正的大佬之时（很遥远），自然就会信心倍增了。之后还有更大的挑战等着我，加油吧。</p><hr><p><font size="4">最后，计算机保研的同学可以加这个群：<strong>605176069</strong></font><br>群友人均保研清北，遇到什么问题可以在里面问问学长学姐。<br><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><img src="/img/hj.png" alt="滑稽"><br>希望每个人的努力都能有所回报！</p>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【机器学习】手撸一个BP神经网络（支持minibatch，Adam优化）</title>
      <link href="/post/5e34cb8d.html"/>
      <url>/post/5e34cb8d.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/img/1.jpg" alt="封面"></p><p>每个函数以及变量的含义已经写在注释中。<br>使用数据：<a href="/download/spambase.csv">点击下载</a><br>测试结果：（第一列为训练集acc，第二列为测试集acc，每行的行数为神经网络层数，每层20个神经元）<br><img src="/img/BP.jpg" alt="结果"></p><p>测试的时候层数不能太深，容易梯度爆炸<br>所需头文件：</p><pre><code>#include &lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;#include&lt;cmath&gt;#include&lt;fstream&gt;#include&lt;sstream&gt;</code></pre><p>BP类定义：</p><pre><code>class BP{private:    vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; w;//需要学习的权重    vector&lt;vector&lt;double&gt;&gt;b;//每层神经网络的偏置    int layers;//层数    vector&lt;int&gt;nl;//每层神经元数量    int m;//样本总数    int act;//激活函数种类    double c;//正则化系数    double a;//学习率    int seed;//随机种子    bool adam;//是否使用adam优化    double testp;//训练集所占比重    vector&lt;vector&lt;double&gt;&gt;za;//记录每个神经元激活值    vector&lt;vector&lt;double&gt;&gt;zz;//记录每个神经元输入值    vector&lt;vector&lt;double&gt;&gt;dz;//记录每个神经元输入的偏导    double forward(double **x, int *y, int q); //第q个样本前向传播    void back(double **x, int *y, int q); //第q个样本反向传播    double relu(double x) { return max(0.0, x); } //以下为三种激活函数以及其导函数    double drelu(double x) { return x &gt;= 0 ? 1 : 0; }    double sigmod(double x) { return 1 / (1 + exp(-x)); }    double dsigmod(double x) { return sigmod(x)*(1 - sigmod(x)); }    double tanh(double x) { return std::tanh(x); }    double dtanh(double x) { return 1 - std::tanh(x)*std::tanh(x); }public:    BP(int layers, int m, int *nl, double a, double c, string act = &quot;relu&quot;, double testp = 0.7, bool adam = false, int seed = 0);    void init();    void train(double **x, int *y, int minibatch); //开始训练    double testacc(double**x, int *y); //计算测试集acc    double trainacc(double**x, int *y);//计算训练集acc    void norminput(double**x); //正则化输入};</code></pre><p>下面给出每个函数的定义：<br>构造函数，传进来层数不包括最后的输出层，所以在构造函数里加上，假设是二分类，所以最后一层只需要一个神经元。</p><pre><code>BP::BP(int layers, int m, int *nl, double a, double c, string act, double testp, bool adam, int seed){    this-&gt;layers = layers + 1;    this-&gt;m = m;    for (int i = 0; i &lt; layers; i++)        this-&gt;nl.emplace_back(nl[i]);    this-&gt;nl.emplace_back(1);    if (act == &quot;relu&quot;)        this-&gt;act = 1;    else if (act == &quot;tanh&quot;)        this-&gt;act = 2;    else if (act == &quot;sigmod&quot;)        this-&gt;act = 3;    this-&gt;seed = seed;    this-&gt;a = a;    this-&gt;c = c;    this-&gt;adam = adam;    this-&gt;testp = testp;}</code></pre><p>初始化函数，各种申请空间和随机初始化，不解释</p><pre><code>void BP::init(){    srand(seed);    w = vector&lt; vector&lt;vector&lt;double&gt;&gt;&gt;(layers - 1);    for (int i = 0; i &lt; layers - 1; i++)    {        w[i] = vector&lt;vector&lt;double&gt;&gt;(nl[i + 1]);        for (int j = 0; j &lt; nl[i + 1]; j++)        {            w[i][j] = vector&lt;double&gt;(nl[i]);            for (int k = 0; k &lt; nl[i]; k++)                w[i][j][k] = rand() % 1000 / (double)100000 - 0.005;        }    }    b = vector&lt;vector&lt;double&gt;&gt;(layers - 1);    for (int i = 0; i &lt; layers - 1; i++)    {        b[i] = vector&lt;double&gt;(nl[i + 1]);        for (int j = 0; j &lt; nl[i + 1]; j++)            b[i][j] = rand() % 1000 / (double)100000 - 0.005;    }    za = vector&lt;vector&lt;double&gt;&gt;(layers);    for (int i = 0; i &lt; layers; i++)        za[i] = vector&lt;double&gt;(nl[i]);    zz = vector&lt;vector&lt;double&gt;&gt;(layers);    for (int i = 0; i &lt; layers; i++)        zz[i] = vector&lt;double&gt;(nl[i]);    dz = vector&lt;vector&lt;double&gt;&gt;(layers);    for (int i = 0; i &lt; layers; i++)        dz[i] = vector&lt;double&gt;(nl[i]);}</code></pre><p>单个样本前向传播，第一层就是x的输入，然后每层每个神经元依次计算输入与激活值，返回该样本的log损失误差，最后一层用sigmod计活函数</p><pre><code>double BP::forward(double **x, int *y, int q){    for (int i = 0; i &lt; nl[0]; i++)        za[0][i] = x[q][i];    for (int i = 0; i &lt; layers - 1; i++)    {        for (int j = 0; j &lt; nl[i + 1]; j++)        {            double z = 0;            for (int k = 0; k &lt; nl[i]; k++)                z += w[i][j][k] * za[i][k];            z += b[i][j];            zz[i + 1][j] = z;            if (i == layers - 2)                z = sigmod(z);            else            {                switch (act)                {                case 1:z = relu(z); break;                case 2:z = tanh(z); break;                case 3:z = sigmod(z); break;                }            }            za[i + 1][j] = z;        }    }    double J = y[q] * log(za[layers - 1][0] + 1e-8) + (1 - y[q])*log(1 - za[layers - 1][0] + 1e-8);    return J;}</code></pre><p>单个样本反向传播求导，最后一层的导数并不是定义为<code>a-y</code>，而是通过损失函数对a的导与sigmod函数对z的导相乘化简而来的</p><pre><code>void BP::back(double**x, int *y, int q){    dz[layers - 1][0] = za[layers - 1][0] - y[q];    for (int i = layers - 2; i &gt; 0; i--)    {        for (int j = 0; j &lt; nl[i]; j++)        {            double dd = 0;            for (int k = 0; k &lt; nl[i + 1]; k++)                dd += w[i][k][j] * dz[i + 1][k];            switch (act)            {            case 1:dd *= drelu(zz[i][j]); break;            case 2:dd *= dtanh(zz[i][j]); break;            case 3:dd *= sigmod(zz[i][j]); break;            }            dz[i][j] = dd;        }    }}</code></pre><p>训练函数，如果minibatch大于m，就直接等于m，dnum为一轮的次数，每求完一轮，计算损失函数，与上一轮相比，相差不大于1e-6，就结束训练,如果训练200轮还没收敛，也结束训练<br>其中dw为损失函数对每层的w的偏导，db同理，vdw与vdb为dw与db的指数加权平均数，sdw与sdb为(dw)²与(db)²的指数加权平均数，这四个变量用来做adam优化的梯度下降</p><pre><code>void BP::train(double **x, int *y, int minibatch){    vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; dw(layers - 1);    for (int i = 0; i &lt; layers - 1; i++)    {        dw[i] = vector&lt;vector&lt;double&gt;&gt;(nl[i + 1]);        for (int j = 0; j &lt; nl[i + 1]; j++)            dw[i][j] = vector&lt;double&gt;(nl[i]);    }    vector&lt;vector&lt;double&gt;&gt;db(layers - 1);    for (int i = 0; i &lt; layers - 1; i++)        db[i] = vector&lt;double&gt;(nl[i + 1]);    vector &lt;vector&lt;vector&lt;double&gt;&gt;&gt;vdw(layers - 1);    for (int i = 0; i &lt; layers - 1; i++)    {        vdw[i] = vector&lt;vector&lt;double&gt;&gt;(nl[i + 1]);        for (int j = 0; j &lt; nl[i + 1]; j++)            vdw[i][j] = vector&lt;double&gt;(nl[i], 0);    }    vector&lt;vector&lt;double&gt;&gt;vdb(layers - 1);    for (int i = 0; i &lt; layers - 1; i++)        vdb[i] = vector&lt;double&gt;(nl[i + 1], 0);    vector &lt;vector&lt;vector&lt;double&gt;&gt;&gt;sdw(layers - 1);    for (int i = 0; i &lt; layers - 1; i++)    {        sdw[i] = vector&lt;vector&lt;double&gt;&gt;(nl[i + 1]);        for (int j = 0; j &lt; nl[i + 1]; j++)            sdw[i][j] = vector&lt;double&gt;(nl[i], 0);    }    vector&lt;vector&lt;double&gt;&gt;sdb(layers - 1);    for (int i = 0; i &lt; layers - 1; i++)        sdb[i] = vector&lt;double&gt;(nl[i + 1], 0);    double Jlast = 0;    double Jnow = 100000;    int t = 1;    int dnum = (int)(m*testp) % minibatch == 0 ? (m*testp) / minibatch : (m*testp) / minibatch + 1;    int anum = 0;    while (abs(Jlast - Jnow) &gt; 1e-5)    {        Jlast = Jnow;        Jnow = 0;        int m1 = 0, m2 = min((int)(m*testp), m1 + minibatch);        for (int ci = 0; ci &lt; dnum; ci++)        {            for (int i = 0; i &lt; layers - 1; i++)                for (int j = 0; j &lt; nl[i + 1]; j++)                    for (int k = 0; k &lt; nl[i]; k++)                        dw[i][j][k] = 0;            for (int i = 1; i &lt; layers; i++)                for (int j = 0; j &lt; nl[i]; j++)                    db[i - 1][j] = 0;            //cout &lt;&lt; &quot;epoch &quot; &lt;&lt; t/dnum +1 &lt;&lt; &quot;: &quot; &lt;&lt; ci + 1 &lt;&lt; &quot;/&quot; &lt;&lt; dnum &lt;&lt; endl;            if (ci != 0)                m1 += minibatch, m2 = min((int)(m*testp), m2 + minibatch);            double J = 0;            for (int i = m1; i &lt; m2; i++)            {                double j = forward(x, y, i);                //cout &lt;&lt; j &lt;&lt; endl;                J += j;                back(x, y, i);                for (int i = 0; i &lt; layers - 1; i++)                    for (int j = 0; j &lt; nl[i + 1]; j++)                        for (int k = 0; k &lt; nl[i]; k++)                            dw[i][j][k] += dz[i + 1][j] * za[i][k];                for (int i = 1; i &lt; layers; i++)                    for (int j = 0; j &lt; nl[i]; j++)                        db[i - 1][j] += dz[i][j];            }            J /= -(double)(m2 - m1);            Jnow += J;            for (int i = 0; i &lt; layers - 1; i++)                for (int j = 0; j &lt; nl[i + 1]; j++)                    for (int k = 0; k &lt; nl[i]; k++)                    {                        dw[i][j][k] = dw[i][j][k] / (double)(m2 - m1) + w[i][j][k] * c / (m2 - m1);                        if (!adam)                            w[i][j][k] -= a * dw[i][j][k];                        else                        {                            vdw[i][j][k] = 0.9*vdw[i][j][k] + 0.1*dw[i][j][k];                            sdw[i][j][k] = 0.999*sdw[i][j][k] + 0.001*dw[i][j][k] * dw[i][j][k];                            double vdwt = vdw[i][j][k] / (1 - pow(0.9, t));                            double sdwt = sdw[i][j][k] / (1 - pow(0.999, t));                            w[i][j][k] -= a * vdwt / sqrt(sdwt + 1e-8);                        }                    }            for (int i = 1; i &lt; layers; i++)                for (int j = 0; j &lt; nl[i]; j++)                {                    db[i - 1][j] = db[i - 1][j] / (double)(m2 - m1);                    if (!adam)                        b[i - 1][j] -= a * db[i - 1][j];                    else                    {                        vdb[i - 1][j] = 0.9*vdb[i - 1][j] + 0.1*db[i - 1][j];                        sdb[i - 1][j] = 0.999*sdb[i - 1][j] + 0.001*db[i - 1][j] * db[i - 1][j];                        double vdbt = vdb[i - 1][j] / (1 - pow(0.9, t));                        double sdbt = sdb[i - 1][j] / (1 - pow(0.999, t));                        b[i - 1][j] -= a * vdbt / (sqrt(sdbt + 1e-8));                        //cout &lt;&lt; db[i - 1][j] &lt;&lt; &quot; &quot; &lt;&lt; vdbt / (sqrt(sdbt + 1e-8)) &lt;&lt; endl;                    }                }            t++;        }        //cout &lt;&lt; Jnow &lt;&lt; endl;        double JC = 0;        for (int i = 0; i &lt; layers - 1; i++)            for (int j = 0; j &lt; nl[i + 1]; j++)                for (int k = 0; k &lt; nl[i]; k++)                    JC += w[i][j][k] * w[i][j][k];        JC *= c;        JC /= 2 * (m*testp);        Jnow += JC;        //printf(&quot;%lf %lf\n&quot;, Jnow, Jlast);        if (t / dnum &gt;= 200)            break;    }}</code></pre><p>两个测试函数，没啥好说的</p><pre><code>double BP::testacc(double**x, int *y){    int m1 = (int)(m*testp);    int tr = 0;    for (int i = m1; i &lt; m; i++)    {        forward(x, y, i);        if (za[layers - 1][0] &gt;= 0.5&amp;&amp;y[i] == 1)            tr++;        if (za[layers - 1][0] &lt; 0.5&amp;&amp;y[i] == 0)            tr++;    }    return tr / (double)(m - m1);}double BP::trainacc(double**x, int *y){    int tr = 0;    for (int i = 0; i &lt; (int)(m*testp); i++)    {        forward(x, y, i);        if (za[layers - 1][0] &gt;= 0.5&amp;&amp;y[i] == 1)            tr++;        if (za[layers - 1][0] &lt; 0.5&amp;&amp;y[i] == 0)            tr++;    }    return tr / (double)(m*testp);}</code></pre><p>正则化输入，每个x减去均值并除以范围（标准差也可以）</p><pre><code>void BP::norminput(double**x){    for (int i = 0; i &lt; nl[0]; i++)    {        double avg = 0;        double maxx = 0;        double minn = 99999999;        for (int j = 0; j &lt; m; j++)        {            avg += x[j][i];            maxx = max(maxx, x[j][i]);            minn = min(minn, x[j][i]);        }        avg /= m;        for (int j = 0; j &lt; m; j++)            x[j][i] = (x[j][i] - avg) / (maxx - minn);    }}</code></pre><p>最后是主函数，读入文件，尝试测试不同层数的神经网络</p><pre><code>int main() {    ifstream f;    f.open(&quot;spambase.csv&quot;);    if (!f.is_open())    {        cout &lt;&lt; &quot;open error&quot; &lt;&lt; endl;        return 0;    }    string line;    int m = 0;    int n = 0;    double **x = new double*[10000];    int *y = new int[10000];    while (getline(f, line))    {        int n1 = 0;        x[m] = new double[1000];        istringstream sin(line);        string temp;        while (getline(sin, temp, &#39;,&#39;))        {            x[m][n1] = atof(temp.c_str());            //cout &lt;&lt; x[m][n1] &lt;&lt; endl;            n1++;        }        y[m] = x[m][n1 - 1] == -1 ? 0 : 1;        m++;        n = n1 - 1;    }    f.close();    int *nl = new int[11];    nl[0] = n;    nl[1] = 20;    nl[2] = 20;    nl[3] = 20;    nl[4] = 20;    nl[5] = 20;    nl[6] = 20;    for (int i = 1; i &lt;= 6; i++)    {        BP B(i, m, nl, 0.01, 0.0001, &quot;tanh&quot;, 0.7, true, 0);        B.init();        B.norminput(x);        B.train(x, y, 64);        double trainacc = B.trainacc(x, y);        double acc = B.testacc(x, y);        printf(&quot;%lf %lf\n&quot;, trainacc, acc);        //system(&quot;pause&quot;);    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撸 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【机器学习】手撸一个逻辑回归</title>
      <link href="/post/723f47a8.html"/>
      <url>/post/723f47a8.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>flag为false时为测试集，为true时为训练集</p><p>J函数：logistics代价函数，为对数代价。</p><p>logistics函数：x特征，y标签，m1样本起始点，m2样本结束点，n特征数，a学习率，c正则化参数，返回n+1长度的数组，为训练好的参数。</p><p>kfold函数：x特征，y标签，n特征数，m样本数，k折数，返回k折平均acc。</p><p>测试数据： <a href="/download/breast-w.csv">点击下载</a><br>数据说明：第10列为标签，-1与1，前九列为特征。</p><p>代码运行结果：（5折交叉验证）<br><img src="/img/5.png" alt="结果"><br>代码：</p><pre><code>#include &lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;algorithm&gt;#include&lt;string&gt;#include&lt;vector&gt;#include&lt;stack&gt;#include&lt;cmath&gt;#include&lt;set&gt;#include&lt;string.h&gt;#include&lt;fstream&gt;#include&lt;sstream&gt;#include&lt;ctime&gt;using namespace std;bool flag[10000];double J(double *s, double **x, int *y,int m1, int m2, int n,double c){    double res = 0;    int fa = 0;    for (int i = m1; i &lt; m2; i++)    {        if (!flag[i])        {            fa++;            continue;        }        double z = -s[0];        for (int j = 1; j &lt;= n; j++)            z -= s[j] * x[i][j-1];        //cout &lt;&lt; z &lt;&lt; endl;        double h = (double)1 / (1 + exp(z));        //cout &lt;&lt; h &lt;&lt; endl;        double cost = 0;        if (y[i] == 1)            cost = -log(h);        else            cost = -log(1 - h);        //cout &lt;&lt; cost &lt;&lt; endl;        res += cost;    }    res /= m2-m1-fa;    double res2 = 0;    for (int i = 1; i &lt;= n; i++)        res2 += s[i] * s[i];    res2 *= c;    res2 /= 2 * (m2 - m1 - fa);    return res + res2;}double* logistic(double **x, int *y, int n,int m1, int m2,double a,double c){    double *s = new double[n + 1];//θ    srand(time(0));    for (int i = 0; i &lt; n + 1; i++)        s[i] = rand()%100/(double)100;//随机初始化    cout &lt;&lt; s[0] &lt;&lt; endl;    double last = 0;    double now = J(s, x, y, m1,m2, n, c);    double *temp = new double[n + 1];    while (abs(last - now) &gt; 0.00001)    {        //cout &lt;&lt;  now &lt;&lt; endl;        for (int i = 0; i &lt;= n; i++)        {            double mqiuhe = 0;            int fa = 0;            for (int k = m1; k &lt; m2; k++)            {                if (!flag[k])                {                    fa++;                    continue;                }                double z = -s[0];                for (int j = 1; j &lt;= n; j++)                    z -= s[j] * x[k][j-1];                double h = (double)1 / (1 + exp(z));                h -= y[k];                if(i!=0)                    h *= x[k][i-1];                mqiuhe += h;            }            mqiuhe /= m2-m1-fa;            mqiuhe *= a;            double C = 0;            if(i!=0)                C = c / (m2 - m1 - fa)*s[i],C *= a;            temp[i] = s[i] - mqiuhe - C;        }        for (int i = 0; i &lt;= n; i++)            s[i] = temp[i];        last = now;        now = J(s, x, y, m1,m2, n, c);    }    return s;}double kfold(double **x, int *y, int n, int m, int k){    double avg = 0;    double d = m / k;    for (int j = 0; j &lt; k; j++)    {        for (int i = 0; i &lt; m; i++)            flag[i] = true;        for (int i = d * j; i &lt; d*j + d; i++)//测试集            flag[i] = false;        double *s = logistic(x, y, n, 0, m, 0.1, 0.01);//学习率0.1，正则化权重0.01        int tr = 0;        for (int i = d * j; i &lt; d*j + d; i++)        {            double res = s[0];            for (int j = 1; j &lt;= n; j++)                res += s[j] * x[i][j - 1];            double h = (double)1 / (1 + exp(-res));            if (h &gt;= 0.5&amp;&amp;y[i] == 1)                tr++;            else if (h &lt; 0.5&amp;&amp;y[i] == 0)                tr++;            //cout &lt;&lt; h &lt;&lt; endl;        }        //cout &lt;&lt; tr &lt;&lt; endl;        double acc = tr / (double)d;        avg += acc;        cout &lt;&lt; &quot;第&quot; &lt;&lt; j +1 &lt;&lt; &quot;次acc值为：&quot; &lt;&lt; acc &lt;&lt; endl;    }    return avg / k;}int main() {    ifstream f;    f.open(&quot;breast-w.csv&quot;);    if (!f.is_open())    {        cout &lt;&lt; &quot;open error&quot; &lt;&lt; endl;        return 0;    }    string line;    int m = 0;    int n = 0;    double **x = new double*[10000];    int *y = new int[10000];    while (getline(f,line))       {            int n1 = 0;        x[m] = new double[1000];        istringstream sin(line);         string temp;        while (getline(sin, temp, &#39;,&#39;))         {            x[m][n1] = atof(temp.c_str());            //cout &lt;&lt; x[m][n1] &lt;&lt; endl;            n1++;        }        y[m] = x[m][n1 - 1]==-1?0:1;        m++;        n = n1-1;    }    double acc = kfold(x, y, n, m, 5);    cout &lt;&lt; &quot;平均：&quot; &lt;&lt; acc &lt;&lt; endl;}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撸 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【天梯赛】第四届团体程序设计天梯赛L2题解</title>
      <link href="/post/c332d4b5.html"/>
      <url>/post/c332d4b5.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/img/4.jpg" alt="封面"></p><h1 id="L2-1-特立独行的幸福-（25分"><a href="#L2-1-特立独行的幸福-（25分" class="headerlink" title="L2-1 特立独行的幸福 （25分)"></a>L2-1 特立独行的幸福 （25分)</h1><p>对一个十进制数的各位数字做一次平方和，称作一次迭代。如果一个十进制数能通过若干次迭代得到 1，就称该数为幸福数。1 是一个幸福数。此外，例如 19 经过 1 次迭代得到 82，2 次迭代后得到 68，3 次迭代后得到 100，最后得到 1。则 19 就是幸福数。显然，在一个幸福数迭代到 1 的过程中经过的数字都是幸福数，它们的幸福是依附于初始数字的。例如 82、68、100 的幸福是依附于 19 的。而一个特立独行的幸福数，是在一个有限的区间内不依附于任何其它数字的；其独立性就是依附于它的的幸福数的个数。如果这个数还是个素数，则其独立性加倍。例如 19 在区间[1, 100] 内就是一个特立独行的幸福数，其独立性为 2×4=8。</p><p>另一方面，如果一个大于1的数字经过数次迭代后进入了死循环，那这个数就不幸福。例如 29 迭代得到 85、89、145、42、20、4、16、37、58、89、…… 可见 89 到 58 形成了死循环，所以 29 就不幸福。</p><p>本题就要求你编写程序，列出给定区间内的所有特立独行的幸福数和它的独立性。</p><h2 id="输入格式："><a href="#输入格式：" class="headerlink" title="输入格式："></a>输入格式：</h2><p>输入在第一行给出闭区间的两个端点：1&lt;A&lt;B≤10^4</p><h2 id="输出格式："><a href="#输出格式：" class="headerlink" title="输出格式："></a>输出格式：</h2><p>按递增顺序列出给定闭区间 [A,B] 内的所有特立独行的幸福数和它的独立性。每对数字占一行，数字间以 1 个空格分隔。</p><p>如果区间内没有幸福数，则在一行中输出 SAD。</p><h2 id="输入样例-1："><a href="#输入样例-1：" class="headerlink" title="输入样例 1："></a>输入样例 1：</h2><pre><code>10 40</code></pre><h2 id="输出样例-1："><a href="#输出样例-1：" class="headerlink" title="输出样例 1："></a>输出样例 1：</h2><pre><code>19 823 628 331 432 3</code></pre><p>注意：样例中，10、13 也都是幸福数，但它们分别依附于其他数字（如 23、31 等等），所以不输出。其它数字虽然其实也依附于其它幸福数，但因为那些数字不在给定区间 [10, 40] 内，所以它们在给定区间内是特立独行的幸福数。</p><h2 id="输入样例-2："><a href="#输入样例-2：" class="headerlink" title="输入样例 2："></a>输入样例 2：</h2><pre><code>110 120</code></pre><h2 id="输出样例-2："><a href="#输出样例-2：" class="headerlink" title="输出样例 2："></a>输出样例 2：</h2><pre><code>SAD</code></pre><p>思路：从A开始遍历，到B结束，如果当前数是幸福数，加入结果集合total，并保存当前数和其独立性，此后每次遍历其他数的时候，如果过程中得到了结果集合里的数，说明之前的数依赖当前的数，则删掉之前的数。每次遍历的时候再用一个集合判断是否循环。遍历结束时，把之前曾经是幸福数的每个数都遍历一下，出现在结果集合里的则是最终答案。</p><pre><code>#include &lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;algorithm&gt;#include &lt;string&gt;#include &lt;vector&gt;#include&lt;stack&gt;#include&lt;cmath&gt;#include&lt;set&gt;#include&lt;string.h&gt;using namespace std;int su(int n){    int j = sqrt(n);    for (int i = 2; i &lt;= j; i++)        if (n%i == 0)            return 1;    return 2;}int ping(int n){    int res = 0;    for (int i = 0;; i++)    {        res += (n % 10)*(n % 10);        n /= 10;        if (n == 0)            break;    }    return res;}int main() {    int x, y;    cin &gt;&gt; x &gt;&gt; y;    int res[10001];    int w[10001];    int geshu = 0;    set&lt;int&gt; total;    bool yilai[10001];    memset(yilai, false, sizeof(yilai));    for (int i = x; i &lt;= y; i++)    {        if (yilai[i])            continue;        int temp = i;        set&lt;int&gt; bianhuan;        bool yes = false;        if (i == 1)            yes = true;        while (temp != 1)        {            bianhuan.insert(temp);            temp = ping(temp);            if (bianhuan.count(temp) == 1)                break;            if (temp == 1)            {                yes = true;                break;            }            yilai[temp] = true;            if (total.count(temp) == 1)                total.erase(temp);        }        if (yes)        {            res[geshu] = i;            w[geshu] = su(i)*bianhuan.size();            total.insert(i);            geshu++;        }    }    if (geshu == 0)        cout &lt;&lt; &quot;SAD&quot;;    else    {        for(int i=0;i&lt;geshu;i++)            if (total.count(res[i]) == 1)            {                cout &lt;&lt; res[i] &lt;&lt; &quot; &quot; &lt;&lt; w[i] &lt;&lt; endl;            }    }}</code></pre><hr><h1 id="L2-2-冰岛人"><a href="#L2-2-冰岛人" class="headerlink" title="L2-2 冰岛人"></a>L2-2 冰岛人</h1><p>本人到现在还不知道到底该怎么做，改来改去也只有20分，等以后想到了解决方法再来更新。</p><hr><h1 id="L2-3-深入虎穴-（25分"><a href="#L2-3-深入虎穴-（25分" class="headerlink" title="L2-3 深入虎穴 （25分)"></a>L2-3 深入虎穴 （25分)</h1><p>著名的王牌间谍 007 需要执行一次任务，获取敌方的机密情报。已知情报藏在一个地下迷宫里，迷宫只有一个入口，里面有很多条通路，每条路通向一扇门。每一扇门背后或者是一个房间，或者又有很多条路，同样是每条路通向一扇门…… 他的手里有一张表格，是其他间谍帮他收集到的情报，他们记下了每扇门的编号，以及这扇门背后的每一条通路所到达的门的编号。007 发现不存在两条路通向同一扇门。</p><p>内线告诉他，情报就藏在迷宫的最深处。但是这个迷宫太大了，他需要你的帮助 —— 请编程帮他找出距离入口最远的那扇门。</p><h2 id="输入格式：-1"><a href="#输入格式：-1" class="headerlink" title="输入格式："></a>输入格式：</h2><p>输入首先在一行中给出正整数 N（&lt;10^5），是门的数量。最后 N 行，第 i 行（1≤i≤N）按以下格式描述编号为 i 的那扇门背后能通向的门：<br><code>K D[1] D[2] ... D[K]</code><br>其中<code>K</code>是通道的数量，其后是每扇门的编号。</p><h2 id="输出格式：-1"><a href="#输出格式：-1" class="headerlink" title="输出格式："></a>输出格式：</h2><p>在一行中输出距离入口最远的那扇门的编号。题目保证这样的结果是唯一的。</p><h2 id="输入样例："><a href="#输入样例：" class="headerlink" title="输入样例："></a>输入样例：</h2><pre><code>133 2 3 42 5 61 71 81 902 11 101 13001 1200</code></pre><h2 id="输出样例："><a href="#输出样例：" class="headerlink" title="输出样例："></a>输出样例：</h2><pre><code>12</code></pre><p>思路：比赛的时候这道题我用了三种方法写，bfs，迪杰斯特拉，dfs，但最后最高也只得了18分，最后才知道，并不是默认1号门为入口，要找出入度为0的门作为入口，坑啊。之后直接dfs就行了，挺简单的。</p><pre><code>#include &lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;algorithm&gt;#include&lt;string&gt;#include&lt;vector&gt;#include&lt;stack&gt;#include&lt;cmath&gt;#include&lt;set&gt;#include&lt;string.h&gt;using namespace std;vector&lt;int&gt; v[100001];int maxx = 0;int bian = 0;void dfs(int i, int ju){    if (ju &gt; maxx)        maxx = ju, bian = i;    for (int j = 0; j &lt; v[i].size(); j++)        dfs(v[i][j], ju + 1);}int du[100001];int main() {    memset(du, 0, sizeof(du));    int n;    cin &gt;&gt; n;    for (int i = 1; i &lt;= n; i++)    {        int k, t;        cin &gt;&gt; k;        for (int j = 0; j &lt; k; j++)        {            cin &gt;&gt; t;            v[i].push_back(t);            du[t]++;        }    }    int start = 0;    for(int i=1;i&lt;=n;i++)        if (du[i] == 0)        {            start = i;            break;        }    dfs(start, 1);    cout &lt;&lt; bian;}</code></pre><hr><h1 id="L2-4-彩虹瓶-（25分"><a href="#L2-4-彩虹瓶-（25分" class="headerlink" title="L2-4 彩虹瓶 （25分)"></a>L2-4 彩虹瓶 （25分)</h1><p>彩虹瓶的制作过程（并不）是这样的：先把一大批空瓶铺放在装填场地上，然后按照一定的顺序将每种颜色的小球均匀撒到这批瓶子里。</p><p>假设彩虹瓶里要按顺序装 N 种颜色的小球（不妨将顺序就编号为 1 到 N）。现在工厂里有每种颜色的小球各一箱，工人需要一箱一箱地将小球从工厂里搬到装填场地。如果搬来的这箱小球正好是可以装填的颜色，就直接拆箱装填；如果不是，就把箱子先码放在一个临时货架上，码放的方法就是一箱一箱堆上去。当一种颜色装填完以后，先看看货架顶端的一箱是不是下一个要装填的颜色，如果是就取下来装填，否则去工厂里再搬一箱过来。</p><p>如果工厂里发货的顺序比较好，工人就可以顺利地完成装填。例如要按顺序装填 7 种颜色，工厂按照 7、6、1、3、2、5、4 这个顺序发货，则工人先拿到 7、6 两种不能装填的颜色，将其按照 7 在下、6 在上的顺序堆在货架上；拿到 1 时可以直接装填；拿到 3 时又得临时码放在 6 号颜色箱上；拿到 2 时可以直接装填；随后从货架顶取下 3 进行装填；然后拿到 5，临时码放到 6 上面；最后取了 4 号颜色直接装填；剩下的工作就是顺序从货架上取下 5、6、7 依次装填。</p><p>但如果工厂按照 3、1、5、4、2、6、7 这个顺序发货，工人就必须要愤怒地折腾货架了，因为装填完 2 号颜色以后，不把货架上的多个箱子搬下来就拿不到 3 号箱，就不可能顺利完成任务。</p><p>另外，货架的容量有限，如果要堆积的货物超过容量，工人也没办法顺利完成任务。例如工厂按照 7、6、5、4、3、2、1 这个顺序发货，如果货架够高，能码放 6 只箱子，那还是可以顺利完工的；但如果货架只能码放 5 只箱子，工人就又要愤怒了……</p><p>本题就请你判断一下，工厂的发货顺序能否让工人顺利完成任务。</p><h2 id="输入格式：-2"><a href="#输入格式：-2" class="headerlink" title="输入格式："></a>输入格式：</h2><p>输入首先在第一行给出 3 个正整数，分别是彩虹瓶的颜色数量 N（1&lt;N≤10^3）、临时货架的容量 M（&lt;N）、以及需要判断的发货顺序的数量 K。</p><p>随后 K 行，每行给出 N 个数字，是 1 到N 的一个排列，对应工厂的发货顺序。</p><p>一行中的数字都以空格分隔。</p><h2 id="输出格式：-2"><a href="#输出格式：-2" class="headerlink" title="输出格式："></a>输出格式：</h2><p>对每个发货顺序，如果工人可以愉快完工，就在一行中输出 YES；否则输出 NO。</p><h2 id="输入样例：-1"><a href="#输入样例：-1" class="headerlink" title="输入样例："></a>输入样例：</h2><pre><code>7 5 37 6 1 3 2 5 43 1 5 4 2 6 77 6 5 4 3 2 1</code></pre><h2 id="输出样例：-1"><a href="#输出样例：-1" class="headerlink" title="输出样例："></a>输出样例：</h2><pre><code>YESNONO</code></pre><p>思路：没啥好说的，直接读数，用一个stack模拟一下就行了。</p><pre><code>#include &lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;algorithm&gt;#include &lt;string&gt;#include &lt;vector&gt;#include&lt;stack&gt;#include&lt;cmath&gt;#include&lt;set&gt;#include&lt;string.h&gt;using namespace std;int main() {    int n, m, k;    cin &gt;&gt; n &gt;&gt; m &gt;&gt; k;    for (int i = 0; i &lt; k; i++)    {        int a[1001];        for (int j = 0; j &lt; n; j++)            cin &gt;&gt; a[j];        stack&lt;int&gt; s;        int now = 1;        for (int j = 0; j &lt; n; j++)        {            if (a[j] == now)            {                now++;                if (s.empty())                    continue;                while (!s.empty())                {                    if (s.top() == now)                    {                        now++;                        s.pop();                    }                    else                        break;                }            }            else            {                s.push(a[j]);                if (s.size() &gt; m)                    break;            }        }        if (now == n + 1)            cout &lt;&lt; &quot;YES&quot; &lt;&lt; endl;        else            cout &lt;&lt; &quot;NO&quot; &lt;&lt; endl;    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> oj </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
            <tag> oj </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Unity】自制unity3D弹幕游戏</title>
      <link href="/post/2cf7b827.html"/>
      <url>/post/2cf7b827.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/img/3.jpg" alt="封面"><br>学校的Unity3D企业实习的作品，用了三天时间制作</p><h1 id="WSAD上下左右"><a href="#WSAD上下左右" class="headerlink" title="WSAD上下左右"></a>WSAD上下左右</h1><h1 id="J发射子弹，K大招（只能抵消敌方子弹而不会对地方造成伤害）"><a href="#J发射子弹，K大招（只能抵消敌方子弹而不会对地方造成伤害）" class="headerlink" title="J发射子弹，K大招（只能抵消敌方子弹而不会对地方造成伤害）"></a>J发射子弹，K大招（只能抵消敌方子弹而不会对地方造成伤害）</h1><h1 id="发射子弹过程中按L，可变集中火力形态，同时移动速度变慢，便于躲避子弹"><a href="#发射子弹过程中按L，可变集中火力形态，同时移动速度变慢，便于躲避子弹" class="headerlink" title="发射子弹过程中按L，可变集中火力形态，同时移动速度变慢，便于躲避子弹"></a>发射子弹过程中按L，可变集中火力形态，同时移动速度变慢，便于躲避子弹</h1><p>由于没有设置大招条件，所以可以无限发射大招，比较耍赖，所以为了证明自己的技术，最好不要用大招哦<br>玩家有20滴血，小怪有20滴血，BOSS有900滴血<br>打死小怪加10分，打死BOSS加45分，总分超过1500才能被称为大佬<br><img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"> <img src="/img/hj.png" alt="封面"></p><p><center>滑稽分界线</center><br><a href="https://yoyo-nb.github.io/DanmuGame/" title="DanmuGame" target="_blank" rel="noopener">点这里试玩点这里试玩点这里试玩</a>. </p><p>本垃圾试玩视频（在博客里只能360P垃圾画质，选择1080P画质会直接跳进bilibili观看）</p><iframe width="800" height="450" src="//player.bilibili.com/player.html?aid=45565323&cid=79783064&page=1" frameborder="0" allowfullscreen></iframe>]]></content>
      
      
      <categories>
          
          <category> 搞事 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity3D </tag>
            
            <tag> 游戏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【算导】建堆过程时间复杂度分析</title>
      <link href="/post/d469f3ff.html"/>
      <url>/post/d469f3ff.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>以下堆假设都是最大堆。<br>维护堆的性质的函数：将当前点与左右孩子比较，若孩子更大，将较大的孩子与当前节点交换位置，当前节点被换到的子节点继续用此方法，直到叶节点或不需要交换。<br>若共有n个元素，则每次调用该方法的时间复杂度为O(lgn)。建堆过程需要自底向上将所有元素都调用一遍该方法，看起来时间复杂度应为O(nlgn),但这个上界虽然正确，却不是渐进紧确的，有很大一部分的节点高度并没有达到lgn。</p><p>设n个元素，高度为lgn，堆中高度为h的元素最多有n/2^(h+1)个（h从0开始计数），高度为h的节点调用上述方法的时间复杂度为O（h），则总时间代价为：</p><script type="math/tex; mode=display">\sum_{h=0}^{\left \lfloor lgn \right \rfloor}\left \lceil \frac{n}{2^{h+1}} \right \rceil O(h) = O(n\sum_{h=0}^{\left \lfloor lgn \right \rfloor}\frac{h}{2^{h}})</script><p>等式右边由于在O（）内部，所以常数对结果没有影响，所以分母中2的幂数可以改变，变为2^h。根据级数：当|x|&lt;1时:</p><script type="math/tex; mode=display">\sum_{k=0}^{\infty }x^{k} = \frac{1}{1-x}</script><p>两边同时微分，并乘以x，得：</p><script type="math/tex; mode=display">\sum_{k=0}^{\infty }kx^{k} = \frac{x}{(1-x)^{2}}</script><p>将x等于1/2带入，结果为2。<br>则建堆过程的时间复杂度为</p><script type="math/tex; mode=display">O(n\sum_{h=0}^{\left \lfloor lgn \right \rfloor}\frac{h}{2^{h}}) = O(n\sum_{h=0}^{\infty }\frac{h}{2^{h}}) = O(2n) = O(n)</script><p>因此可以在线性时间内把一个无序数组构造成一个最大堆。</p>]]></content>
      
      
      <categories>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【算导】生日悖论</title>
      <link href="/post/81339208.html"/>
      <url>/post/81339208.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>算导P73</p><h1 id="生日悖论"><a href="#生日悖论" class="headerlink" title="生日悖论"></a>生日悖论</h1><p>屋内有k个人，一年有n天，k达到多少时，两人生日相同的机会能达到50%？<br>至少两人生日相同概率等于1-都不相同概率<br>则k个人都不相同的事件：</p><script type="math/tex; mode=display">B_{k}=\bigcap_{i=1}^{k} A_{i}</script><p>其中，Ai为所有j&lt;i，i与j不同生日的事件。则：</p><script type="math/tex; mode=display">B_{k}=A_{k}\bigcap B_{k-1}</script><p>则有递归式：</p><script type="math/tex; mode=display">P(B_{k})=P(B_{k-1})P(A_{k}|B_{k-1})</script><p>即：假设1到k-1编号的人两两不相同，则1到k的人两两不相同的概率等于1到k-1两两不相同的概率乘以k号与前面所有人不相同的概率，取P(B1)=P(A1)=1<br>则可求P(Bk):<br>过程略（太麻烦了）<br>结果为：-k(k-1)/2n≤ln(1/2)时成立，解得当n=365时，k至少为23。</p>]]></content>
      
      
      <categories>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【算导】分治法求解最大子数组</title>
      <link href="/post/35693.html"/>
      <url>/post/35693.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>任意子数组必然是以下三种情况之一：</p><p>1.完全在a[low…mid]中。</p><p>2.完全在a[mid+1…high]中。</p><p>3.跨越了数组中点。</p><p>若结果为1或2，则可通过递归求解，若结果为3，则可通过一个线性时间复杂度的函数求解：</p><pre><code>FIND-CROSS(a,low,mid,high)    leftsum=0;    sum=0;    for(int i=mid;i&gt;=low;i--)    {        sum+=a[i];        leftsum=max(leftsum,sum);    }    rightsum=0;    sum=0;    for(int i=mid;i&lt;high;i++)    {        sum+=a[i];        rightsum=max(rightsum,sum);    }return leftsum+rightsum</code></pre><p>最后写一个总函数：</p><pre><code>FIND(a,low,high)    if(low==high)        return a[low]    else    {        mid=(low+high)/2        b=FIND(a,low,mid)        c=FIND(a,mid+1,high)        d=FIND-CROSS(a,low,mid,high)        return max(b,c,d);    }</code></pre><p>时间复杂度O(nlogn)</p><p>一个更优化的方法，时间复杂度O(n)<br>从头遍历，每遍历一个数，加到sum中，与当前最大值res比较更新res，当sum小于0时，sum、<br>等于0，相当于把之前的所有结果清零，从下一个数重新比较并继续更新res</p><pre><code>int maxSubArray(vector&lt;int&gt; nums) {    int ge = nums.size();    if (ge == 0)        return 0;    int sum = 0;    int res = nums[0];    for (int i = 0; i &lt; ge; i++)    {        sum += nums[i];        res = max(sum, res);        if (sum &lt; 0)            sum = 0;            }    return res;}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 做题笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算导 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
